<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>http://spark.kanseaveg.xyz</id>
    <title>Yuan&#39;s Library</title>
    <link href="http://spark.kanseaveg.xyz" />
    <updated>2021-10-22T06:24:49.000Z</updated>
    <category term="Spark" />
    <entry>
        <id>http://spark.kanseaveg.xyz/2021/10/22/SparkCore-%E5%B9%BF%E5%91%8A%E4%BD%BF%E7%94%A8%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90/</id>
        <title>SparkCore-广告使用量统计分析</title>
        <link rel="alternate" href="http://spark.kanseaveg.xyz/2021/10/22/SparkCore-%E5%B9%BF%E5%91%8A%E4%BD%BF%E7%94%A8%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90/"/>
        <content type="html">&lt;h3 id=&#34;sparkcore-广告使用量统计分析&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#sparkcore-广告使用量统计分析&#34;&gt;#&lt;/a&gt; SparkCore - 广告使用量统计分析&lt;/h3&gt;
&lt;h5 id=&#34;adsrdddemoscala&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#adsrdddemoscala&#34;&gt;#&lt;/a&gt; AdsRDDdemo.scala&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;package com.kanseaveg.rdd

import org.apache.spark.rdd.RDD
import org.apache.spark.&amp;#123;SparkConf, SparkContext&amp;#125;

/**
 * 案例实操  https://raw.githubusercontent.com/kanseavegs/picgo_images/master/data/%E5%AE%9E%E9%99%85%E6%A1%88%E4%BE%8B-1.png
 * agent.log：时间戳，省份，城市，用户，广告，中间字段使用空格分隔。
 * 需求描述: 统计出每一个省份每个广告被点击数量排行的 Top3
 */
object AdsRDDdemo &amp;#123;
  def main(args: Array[String]): Unit = &amp;#123;

    val conf = new SparkConf().setMaster(&amp;quot;local[*]&amp;quot;).setAppName(&amp;quot;AdsRDDdemo&amp;quot;)
    val sc = new SparkContext(conf)
    //1.从agent.log中获取原始数据：时间戳，省份，城市，用户，广告
    val rawData: RDD[String] = sc.textFile(&amp;quot;datas/agent.log&amp;quot;)
    //2.将原始数据进行初步清晰，方便统计 ： 时间戳，省份，城市，用户，广告 to ((省份，广告),1)
    val dataRdd: RDD[((String, String), Int)] = rawData.map(line =&amp;gt; &amp;#123;
      val eachLine: Array[String] = line.split(&amp;quot; &amp;quot;)
      ((eachLine(1), eachLine(4)), 1)
    &amp;#125;)
    //3.将转换结构后的数据进行分组聚合 ： ((省份，广告),1) to ((省份，广告),sum)
    val reduceRdd: RDD[((String, String), Int)] = dataRdd.reduceByKey(_ + _)
    //4.将聚合的结果进行结构的转换 ((省份，广告),sum) =&amp;gt; (省份，(广告,sum)) ,三种方法均可

    val mapRdd: RDD[(String, (String, Int))] = reduceRdd.map( data =&amp;gt; &amp;#123; (data._1._1,(data._1._2,data._2)) &amp;#125;)
//    val mapRdd: RDD[(String, (String, Int))] = reduceRdd.map(
//      data =&amp;gt; &amp;#123;
//        val tp: (String, String) = data._1
//        (tp._1, (tp._2, data._2))
//      &amp;#125;)
//    val mapRdd: RDD[(String, (String, Int))] = reduceRdd.map &amp;#123; case ((province, ads), sum) =&amp;gt; &amp;#123;
//      (province, (ads, sum))
//    &amp;#125;&amp;#125;



    //5.将转换结构后的数据根据省份进行分组 (省份，[(广告A,sumA),广告B,sumB)])
    val groupRdd: RDD[(String, Iterable[(String, Int)])] = mapRdd.groupByKey()
    //6.将扥组后的数据组内降序排序，取前三名打印到控制台
    val resultRdd: RDD[(String, List[(String, Int)])] = groupRdd.mapValues(iter =&amp;gt; &amp;#123;
      iter.toList.sortBy(-_._2).take(3)
    &amp;#125;) //key不变,对value进行排序 ,-号表示倒序
    resultRdd.collect().foreach(println)


    /**
     * 一行写法
     * sc.textFile(&amp;quot;datas/agent.log&amp;quot;).map( l =&amp;gt; ((l.split(&amp;quot; &amp;quot;)(1),l.split(&amp;quot; &amp;quot;)(4)),1))
     * .reduceByKey(_+_).map(d=&amp;gt;(d._1._1,(d._1._2,d._2))).groupByKey()
     * .mapValues(_.toList.sortBy(-_._2).take(3)).collect().foreach(println)
     */

    /**
     * (省份, 每省前三名(广告x,点击数量))
     * (4,List((12,25), (2,22), (16,22)))
     * (8,List((2,27), (20,23), (11,22)))
     * (6,List((16,23), (24,21), (22,20)))
     * (0,List((2,29), (24,25), (26,24)))
     * (2,List((6,24), (21,23), (29,20)))
     * (7,List((16,26), (26,25), (1,23)))
     * (5,List((14,26), (21,21), (12,21)))
     * (9,List((1,31), (28,21), (0,20)))
     * (3,List((14,28), (28,27), (22,25)))
     * (1,List((3,25), (6,23), (5,22)))
     */

  &amp;#125;

&amp;#125;

&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;hotcategorytop10analysisscala&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#hotcategorytop10analysisscala&#34;&gt;#&lt;/a&gt; HotCategoryTop10Analysis.scala&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;package com.kanseaveg.rdd

import org.apache.spark.rdd.RDD
import org.apache.spark.&amp;#123;SparkConf, SparkContext&amp;#125;

/**
 * top10热门品类分析：
 * 分别统计每个品类点击的次数，下单的次数和支付的次数，并按照（点击总数，下单总数，支付总数）排序
 * （品类，点击总数）（品类，下单总数）（品类，支付总数）
 */
object HotCategoryTop10Analysis &amp;#123;
  def main(args: Array[String]): Unit = &amp;#123;
    val conf = new SparkConf().setMaster(&amp;quot;local[*]&amp;quot;).setAppName(&amp;quot;aaaa&amp;quot;)
    val sc = new SparkContext(conf)
    //1.读取原始日志数据
    val dataRdd: RDD[String] = sc.textFile(&amp;quot;datas/user_visit_action.csv&amp;quot;)
    dataRdd.cache()
    /**
     * 数据中包含用户的 4 种行为：搜索，点击，下单，支付：
     *    1.每一行数据表示用户的一次行为，这个行为只能是 4 种行为的一种
     *    2.如果搜索关键字为 null,表示数据不是搜索数据
     *    3.如果点击的品类 ID 和产品 ID 为-1，表示数据不是点击数据
     *    4.对于下单行为，一次可以下单多个商品，品类ID和产品ID可以是多个，id之间采用逗号分隔，如果本次不是下单行为则数据采用 null表示
     *    5.支付行为和下单行为类似
     *
     * 预处理，数据采用下划线分隔数据
     * 行为日期	用户 ID	Session的ID	页面id	动作的时间点
     * 【搜索关键词	品类ID 商品 ID	一次订单中所有品类ID集合	一次订单中所有商品的ID集合	一次支付中所有品类的ID集合	一次支付中所有商品的ID集合】 城市id
     *    5          6      7       8                   9                       10                      11
     */

    //2.统计品类的点击数量： （品类id-点击数量）
    val clickRdd: RDD[(String, Int)] = dataRdd.filter(data =&amp;gt; &amp;#123;
      val line: Array[String] = data.split(&amp;quot;,&amp;quot;)
      line(6) != &amp;quot;-1&amp;quot; &amp;amp;&amp;amp; line(7) != &amp;quot;-1&amp;quot; //点击的品类 ID 和产品 ID 为-1，表示数据不是点击数据，过滤不是点击的数据
    &amp;#125;).map(data =&amp;gt; (data.split(&amp;quot;,&amp;quot;)(6), 1)).reduceByKey(_ + _) //（品类id-点击数量）


    //3.统计品类的下单数量： （品类id-下单数量）
    val orderRdd: RDD[(String, Int)] = dataRdd.filter(data =&amp;gt; &amp;#123;
      val line: Array[String] = data.split(&amp;quot;,&amp;quot;)
      line(8) != &amp;quot;null&amp;quot; &amp;amp;&amp;amp; line(9) != &amp;quot;null&amp;quot; //品类ID和产品ID可以是多个，如果本次不是下单行为则数据采用 null表示
    &amp;#125;).flatMap(data =&amp;gt; (data.split(&amp;quot;,&amp;quot;)(8).split(&amp;quot;-&amp;quot;).map(category_id =&amp;gt; (category_id, 1)))) //扁平化，将品类id集合打散，然后写成（category_id，1）的形式
      .reduceByKey(_ + _)

    //4.统计品类的支付数量： （品类id-支付数量）
    //支付行为和下单行为类似
    val payRdd: RDD[(String, Int)] = dataRdd.filter(data =&amp;gt; &amp;#123;
      val line: Array[String] = data.split(&amp;quot;,&amp;quot;)
      line(10) != &amp;quot;null&amp;quot; &amp;amp;&amp;amp; line(11) != &amp;quot;null&amp;quot; //品类ID和产品ID可以是多个，如果本次不是支付行为则数据采用 null表示
    &amp;#125;).flatMap(data =&amp;gt; (data.split(&amp;quot;,&amp;quot;)(10).split(&amp;quot;-&amp;quot;).map(category_id =&amp;gt; (category_id, 1)))) //扁平化，将品类id集合打散，然后写成（category_id，1）的形式
      .reduceByKey(_ + _)


    //已获得clickRdd：（品类id-点击数量）orderRdd：（品类id-下单数量）payRdd：（品类id-支付数量），需要（品类id，（点击数量，下单数量，支付数量））形式
    //5.将品类进行排序 取出前10名打印  排序规则：点击-下单-支付  使用元组排序即可（品类id，（点击数量，下单数量，支付数量））
//    val coGroupRdd: RDD[(String, (Iterable[Int], Iterable[Int], Iterable[Int]))] = clickRdd.cogroup(orderRdd, payRdd) //cogroup = connect + group 但是cogroup存在shuffle 性能可能比较低
//    val analysisRdd: RDD[(String, (Int, Int, Int))] = coGroupRdd.mapValues(tp =&amp;gt; &amp;#123;(tp._1.sum, tp._2.sum, tp._3.sum)&amp;#125;) //（品类id，（点击数量，下单数量，支付数量））
//    val resultRdd: Array[(String, (Int, Int, Int))] = analysisRdd.sortBy(_._2, false).take(10) //按照（点击数量，下单数量，支付数量）排序 取前10

    /**
     * cogroup改进
     * （品类id-点击数量）=&amp;gt;（品类id，（点击数量，0，0））
     * （品类id-下单数量）=&amp;gt; （品类id，（点击数量，下单数量，0））
     * （品类id-支付数量）=&amp;gt;（品类id，（点击数量，下单数量，支付数量））
     */
    val rdd1: RDD[(String, (Int, Int, Int))] = clickRdd.map(tp =&amp;gt; (tp._1, (tp._2, 0, 0)))
    val rdd2: RDD[(String, (Int, Int, Int))] = orderRdd.map(tp =&amp;gt; (tp._1, (0, tp._2, 0)))
    val rdd3: RDD[(String, (Int, Int, Int))] = payRdd.map(tp =&amp;gt; (tp._1, (0, 0, tp._2)))
    val totalRdd: RDD[(String, (Int, Int, Int))] = rdd1.union(rdd2).union(rdd3)
    val coGroupReplace: RDD[(String, (Int, Int, Int))] = totalRdd.reduceByKey((a, b) =&amp;gt; (a._1 + b._1, a._2 + b._2, a._3 + b._3))
    val resultRdd: Array[(String, (Int, Int, Int))] = coGroupReplace.sortBy(_._2, false).take(10) //按照（点击数量，下单数量，支付数量）排序 取前10



    //6.采集结果并打印
    resultRdd.foreach(println)

    /**
     * (20,(3104,0,0))
     * (15,(3098,0,0))
     * (17,(3080,0,0))
     * (14,(3079,0,0))
     * (10,(3060,0,0))
     * (13,(3056,0,0))
     * (2,(3054,6044,4046))
     * (5,(3032,0,0))
     * (3,(3024,6044,4046))
     * (7,(3020,0,0))
     *
     */

    sc.stop()
  &amp;#125;

&amp;#125;

&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;hotcategorytop10analysis_advancedscala&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#hotcategorytop10analysis_advancedscala&#34;&gt;#&lt;/a&gt; HotCategoryTop10Analysis_Advanced.scala&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;package com.kanseaveg.rdd

import org.apache.spark.rdd.RDD
import org.apache.spark.util.AccumulatorV2
import org.apache.spark.&amp;#123;SparkConf, SparkContext&amp;#125;

import scala.collection.mutable

/**
 * 数据中包含用户的 4 种行为：搜索，点击，下单，支付：
 *    1.每一行数据表示用户的一次行为，这个行为只能是 4 种行为的一种
 *    2.如果搜索关键字为 null,表示数据不是搜索数据
 *    3.如果点击的品类 ID 和产品 ID 为-1，表示数据不是点击数据
 *    4.对于下单行为，一次可以下单多个商品，品类ID和产品ID可以是多个，id之间采用逗号分隔，如果本次不是下单行为则数据采用 null表示
 *    5.支付行为和下单行为类似
 *
 * 预处理，数据采用下划线分隔数据
 * 行为日期	用户 ID	Session的ID	页面id	动作的时间点
 * 【搜索关键词	品类ID 商品 ID	一次订单中所有品类ID集合	一次订单中所有商品的ID集合	一次支付中所有品类的ID集合	一次支付中所有商品的ID集合】 城市id
 *    5          6      7       8                   9                       10                      11
 */

/**
 * 2.将数据转换结构
 *    点击的场合：（品类id，（1，0，0））
 *    下单的场合：（品类id，（0，1，0））
 *    支付的场合：（品类id，（0，0，1））
 */

/**
 * top10热门品类分析：
 * 分别统计每个品类点击的次数，下单的次数和支付的次数，并按照（点击总数，下单总数，支付总数）排序
 * （品类，点击总数）（品类，下单总数）（品类，支付总数）
 *
 *
 * 优化方法一：由于reduceBykey存在大量shuffle操作，于是优化预处理，提前将数据格式处理好，但是任然存在一个reducebykey shuffle操作
 * 优化方法二：去除reduceBykey，使用累加器
 */
object HotCategoryTop10Analysis_Advanced &amp;#123;
  def main(args: Array[String]): Unit = &amp;#123;
    val conf = new SparkConf().setMaster(&amp;quot;local[*]&amp;quot;).setAppName(&amp;quot;aaaa&amp;quot;)
    val sc = new SparkContext(conf)
    //1.读取原始日志数据
    val dataRdd: RDD[String] = sc.textFile(&amp;quot;datas/user_visit_action.csv&amp;quot;)


    /**
     * 优化方法一：由于reduceBykey存在大量shuffle操作，于是优化预处理，提前将数据格式处理好，但是任然存在一个reducebykey shuffle操作
     */
//    val flatRdd: RDD[(String, (Int, Int, Int))] = dataRdd.flatMap(data =&amp;gt; &amp;#123;
//      val cols = data.split(&amp;quot;,&amp;quot;) //按照列拍扁
//      if (cols(6) != &amp;quot;-1&amp;quot; &amp;amp;&amp;amp; cols(7) != &amp;quot;-1&amp;quot;) &amp;#123;
//        List((cols(6), (1, 0, 0))) //统计品类的点击数量,点击的品类ID和产品ID不为-1，表示点击数据
//      &amp;#125; else if (cols(8) != &amp;quot;null&amp;quot; &amp;amp;&amp;amp; cols(9) != &amp;quot;null&amp;quot;) &amp;#123;
//        cols(8).split(&amp;quot;-&amp;quot;).map(category_id =&amp;gt; (category_id, (0, 1, 0))) //品类ID &amp;quot;1-2-3&amp;quot;
//      &amp;#125; else if (cols(10) != &amp;quot;null&amp;quot; &amp;amp;&amp;amp; cols(11) != &amp;quot;null&amp;quot;) &amp;#123;
//        cols(10).split(&amp;quot;-&amp;quot;).map(category_id =&amp;gt; (category_id, (0, 0, 1))) //品类ID &amp;quot;1-2-3&amp;quot;
//      &amp;#125; else &amp;#123;
//        Nil
//      &amp;#125;
//    &amp;#125;)
//    val resultRdd: RDD[(String, (Int, Int, Int))] = flatRdd.reduceByKey((a, b) =&amp;gt; (a._1 + b._1, a._2 + b._2, a._3 + b._3)) //    ( 品类ID，( 点击数量, 下单数量, 支付数量 ) )

    /**
     * 优化方法二：去除reduceBykey，使用自定义累加器
     */
    val acc = new HotCategoryAccumulator
    sc.register(acc,&amp;quot;HotCategoryAcc&amp;quot;)
    dataRdd.foreach(data =&amp;gt; &amp;#123;
      val cols = data.split(&amp;quot;,&amp;quot;) //按照列拍扁

      if (cols(6) != &amp;quot;-1&amp;quot; &amp;amp;&amp;amp; cols(7) != &amp;quot;-1&amp;quot;) &amp;#123;
        acc.add((cols(6),&amp;quot;click&amp;quot;))
      &amp;#125; else if (cols(8) != &amp;quot;null&amp;quot; &amp;amp;&amp;amp; cols(9) != &amp;quot;null&amp;quot;) &amp;#123;
        cols(8).split(&amp;quot;-&amp;quot;).foreach(cid =&amp;gt; &amp;#123;acc.add((cid,&amp;quot;order&amp;quot;))&amp;#125;)
      &amp;#125; else if (cols(10) != &amp;quot;null&amp;quot; &amp;amp;&amp;amp; cols(11) != &amp;quot;null&amp;quot;) &amp;#123;
        cols(10).split(&amp;quot;-&amp;quot;).foreach(cid =&amp;gt; &amp;#123;acc.add((cid,&amp;quot;pay&amp;quot;))&amp;#125;)
      &amp;#125; else &amp;#123;
        Nil
      &amp;#125;
    &amp;#125;)

    val allHotCategories: mutable.Iterable[HotCategory] = acc.value.map(_._2)
    val resultRdd: List[HotCategory] = allHotCategories.toList.sortWith((left, right) =&amp;gt; &amp;#123;
      if (left.clickCnt &amp;gt; right.clickCnt) true
      else if (left.clickCnt == right.clickCnt) &amp;#123;
        if (left.orderCnt &amp;gt; right.orderCnt) true
        else if (left.orderCnt == right.orderCnt) &amp;#123;
          if (left.payCnt &amp;gt; right.payCnt) true
          else false
        &amp;#125; else false
      &amp;#125; else false
    &amp;#125;).take(10)




    //采集结果并打印
    resultRdd.foreach(println)

    /**
     * (20,(3104,0,0))
     * (15,(3098,0,0))
     * (17,(3080,0,0))
     * (14,(3079,0,0))
     * (10,(3060,0,0))
     * (13,(3056,0,0))
     * (2,(3054,6044,4046))
     * (5,(3032,0,0))
     * (3,(3024,6044,4046))
     * (7,(3020,0,0))
     *
     */

    sc.stop()
  &amp;#125;
  case class HotCategory(cid: String, var clickCnt: Int, var orderCnt: Int, var payCnt: Int)

  /**
   * IN: (品类id，行为类型)
   * OUT: Map[String, HotCategory]
   */
  class HotCategoryAccumulator extends AccumulatorV2[(String,String),mutable.Map[String, HotCategory]] &amp;#123;

    private val hcMap = mutable.Map[String,HotCategory]()

    override def isZero: Boolean = hcMap.isEmpty

    override def copy(): AccumulatorV2[(String, String), mutable.Map[String, HotCategory]] = new HotCategoryAccumulator()

    override def reset(): Unit = hcMap.clear()

    override def add(v: (String, String)): Unit = &amp;#123;
      val cid: String = v._1 //获取品类id
      val actionType: String = v._2 //获取需要累加的类型
      val category: HotCategory = hcMap.getOrElse(cid, HotCategory(cid, 0, 0, 0))

      if (actionType == &amp;quot;click&amp;quot;) &amp;#123;
        category.clickCnt += 1
      &amp;#125; else if (actionType == &amp;quot;order&amp;quot;) &amp;#123;
        category.orderCnt += 1
      &amp;#125; else if (actionType == &amp;quot;pay&amp;quot;) &amp;#123;
        category.payCnt += 1
      &amp;#125;
      hcMap.update(cid,category)
    &amp;#125;

    override def merge(other: AccumulatorV2[(String, String), mutable.Map[String, HotCategory]]): Unit = &amp;#123;
      val map1 = this.hcMap
      val map2 = other.value

      //对于map2中每一个元素
      map2.foreach( data =&amp;gt; &amp;#123;
        val cid: String = data._1
        val hcc: HotCategory = data._2 //other&#39;s Map&#39;s HotCategory
        val category: HotCategory = map1.getOrElse(cid, HotCategory(cid, 0, 0, 0))
        category.clickCnt += hcc.clickCnt
        category.orderCnt += hcc.orderCnt
        category.payCnt += hcc.payCnt
        map1.update(cid, category)
      &amp;#125;)
    &amp;#125;

    override def value: mutable.Map[String, HotCategory] = hcMap
  &amp;#125;


&amp;#125;

&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;hotcategorytop10sessionanalysisscala&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#hotcategorytop10sessionanalysisscala&#34;&gt;#&lt;/a&gt; HotCategoryTop10SessionAnalysis.scala&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;import org.apache.spark.rdd.RDD
import org.apache.spark.&amp;#123;SparkConf, SparkContext&amp;#125;

/**
 * 数据中包含用户的 4 种行为：搜索，点击，下单，支付：
 *    1.每一行数据表示用户的一次行为，这个行为只能是 4 种行为的一种
 *    2.如果搜索关键字为 null,表示数据不是搜索数据
 *    3.如果点击的品类 ID 和产品 ID 为-1，表示数据不是点击数据
 *    4.对于下单行为，一次可以下单多个商品，品类ID和产品ID可以是多个，id之间采用逗号分隔，如果本次不是下单行为则数据采用 null表示
 *    5.支付行为和下单行为类似
 *
 * 预处理，数据采用下划线分隔数据
 * 行为日期	用户 ID	Session的ID	页面id	动作的时间点
 * 【搜索关键词	品类ID 商品 ID	一次订单中所有品类ID集合	一次订单中所有商品的ID集合	一次支付中所有品类的ID集合	一次支付中所有商品的ID集合】 城市id
 *    5          6      7       8                   9                       10                      11
 */

/**
 * top10热门品类分析：
 * Top10 热门品类中每个品类的 Top10 活跃 Session 统计
 */
object HotCategoryTop10SessionAnalysis &amp;#123;
  def main(args: Array[String]): Unit = &amp;#123;
    val conf = new SparkConf().setMaster(&amp;quot;local[*]&amp;quot;).setAppName(&amp;quot;aaaa&amp;quot;)
    val sc = new SparkContext(conf)
    val dataRdd: RDD[String] = sc.textFile(&amp;quot;datas/user_visit_action.csv&amp;quot;)

    val top10Ids: Array[String] = top10Category(dataRdd)
    //过滤Top10热门品类中的点击品类Ids
    val clickCidsOfTop10: RDD[String] = dataRdd.filter(data =&amp;gt; &amp;#123;
      val cols: Array[String] = data.split(&amp;quot;,&amp;quot;)
      cols(6) != &amp;quot;-1&amp;quot; &amp;amp;&amp;amp; cols(7) != &amp;quot;-1&amp;quot; &amp;amp;&amp;amp; top10Ids.contains(cols(6)) //  点击的品类ID和产品 ID不为-1
    &amp;#125;)

    //根据品类id和sessionid进行点击量统计
    val reduceRdd: RDD[((String, String), Int)] = clickCidsOfTop10.map(data =&amp;gt; &amp;#123;
      val cols: Array[String] = data.split(&amp;quot;,&amp;quot;)
      ((cols(6), cols(2)), 1) // ((品类ID，sessionId),1))
    &amp;#125;).reduceByKey(_ + _) //// ((品类ID，sessionId),sum))


    // 将统计的结果进行结构的转换 转换后进行分组聚合  ((品类ID，sessionId),sum)) to  (品类ID，(sessionId,sum)))
    val groupRdd: RDD[(String, Iterable[(String, Int)])] = reduceRdd.map(data =&amp;gt; (data._1._1, (data._1._2, data._2))).groupByKey() //  (品类ID，(sessionId,sum))) 按照品类id分组

    //将分组后的数据进行点击量的排序，取前10名
    val resultRdd: RDD[(String, List[(String, Int)])] = groupRdd.mapValues(iter =&amp;gt; iter.toList.sortBy(_._2)(Ordering.Int.reverse).take(10)) //按照sum排序

    resultRdd.foreach(println)

    /**
     * (4,List((5310bea7-9d61-4c9b-bfd7-7176c40a7d1f,6), (82956a11-86bc-4ab9-a7c5-2c602d5da84d,5), (83df2649-0b85-41d9-a1db-21392954b83d,5), (32f6cee8-4dc9-49e0-a05d-dbc0c1c9daae,5), (fd9a3aae-f5c2-425f-bb8f-8ed19db009fa,4), (9e3b5ab9-ba3e-4334-92df-6d851db2b75f,4), (0fb43e3d-78dd-474f-8892-08fb2c0a5132,4), (b2741f4f-a071-464c-bbc0-baf863fdc767,4), (23087483-70c1-4e5f-9d53-5f7020032929,4), (9e724bb6-8903-45d8-a18e-69e8386e6bad,4)))
     * (8,List((040c420b-6709-4aa3-818e-4bfdfac59640,5), (9e5db7d9-4616-46bd-9cef-de671d113925,5), (9e35a818-784b-4723-8d1d-a19acda88f29,5), (06ad898c-0390-452a-88e7-2ca1c1d3355c,5), (0594ee8f-499d-49c6-8328-aa756467a2ad,5), (b3c63508-d10d-427e-a797-d44b172a40b1,5), (c7f1bea8-94dc-444c-b1e6-4b767c7c4f13,4), (dd89c2b4-d6f1-4112-b705-6ce263abdfd4,4), (57500425-a1d6-4dad-8ec4-224e443e567f,4), (738447c6-e65b-4fd9-8158-d244245b6633,4)))
     * (20,List((710373f5-3a2e-4fec-9ddb-623d779273e6,6), (f632bbe4-e5ba-4dcf-91ef-89f9688376a9,6), (7d0014a4-c501-456f-9d30-fe6af79d933c,6), (3208994d-5867-4c8f-841f-42700aeabfb3,6), (c34d6503-b62b-4bab-9b85-7b0aefd8a5e0,5), (c9f87d6f-6a62-4e20-a2bb-aa0b690c134c,5), (78bc6698-ac26-4f1b-aaa0-05232f3322e5,5), (7ea420e7-eaa4-42dd-b517-470656fb87b6,5), (1c5e83e4-370c-4163-a387-6d61a0233e24,5), (8b9a0f5a-bff9-4f37-b3bb-6aa3c24d24f7,5)))
     * (19,List((8b9a0f5a-bff9-4f37-b3bb-6aa3c24d24f7,7), (72b227ad-aaad-4c6c-965f-d2ed3428aa20,6), (592c3702-9886-4725-9869-a43cf972a7dc,5), (e068b484-7f57-4e7e-9db8-d6842ce43858,5), (6b53902b-93e2-4e08-be23-da9e12699dc5,5), (0700b3ec-90b1-42be-bc08-ae9a0081cfe9,4), (7d0014a4-c501-456f-9d30-fe6af79d933c,4), (dfa98b65-2ebc-4215-b080-0aff2f57d89a,4), (a5975ee5-91b1-4e22-b6f0-d4a12f8fd4ed,4), (577e5749-3594-448c-9771-e5f398a9cbb3,4)))
     * (15,List((8b9a0f5a-bff9-4f37-b3bb-6aa3c24d24f7,8), (e01a941e-08e7-4fd6-aa77-eb10b5c2644b,8), (4c840047-6302-4e7d-8505-f5a0ac11da2f,5), (dd89c2b4-d6f1-4112-b705-6ce263abdfd4,5), (1a163c8d-3d58-4a76-8b28-d732bb51aec8,5), (57500425-a1d6-4dad-8ec4-224e443e567f,5), (0f0c33f2-a9c9-4264-befe-dce0d892526a,5), (f9936a33-3cad-4ac0-8153-77234c6cdb09,5), (d988514f-f1cd-4ddf-84b5-e11fb536e97a,4), (75cf4d14-87f4-4e39-8050-c3d164a002bc,4)))
     * (6,List((8b9a0f5a-bff9-4f37-b3bb-6aa3c24d24f7,7), (bc9b9ad6-cff4-490e-a405-4c5b3b284fcb,6), (a5400810-3480-4fea-ba5a-3c77170082c3,5), (b7a3d1d4-fa2b-48c8-8bec-e07c59e02c6c,5), (f379a991-0ebc-4817-898d-86affa518835,4), (8b3c224f-66f7-48b6-a9e2-a8861968114c,4), (c1d705bf-bfb2-4a19-94af-c32ea0d5885d,4), (49e2529d-ff6d-46a8-9742-42c01170e394,4), (6dc1bfb3-92b4-44c4-b790-5fbc7ca414ed,4), (7328ce83-2c98-4d39-a1bf-1af472bb8c68,4)))
     * (2,List((36003baf-6d64-49be-a047-9fc8c0e76ca8,6), (233e790a-1d96-4559-9205-471dc235161d,5), (1ecc15fb-85e4-4c0f-b68f-7351d9f77895,5), (e222b39c-8b13-4865-a8ba-892bd8919b5b,5), (72b227ad-aaad-4c6c-965f-d2ed3428aa20,5), (e5a2e03a-e650-446e-bff2-4845dc726278,5), (0d10c830-c48c-418c-b51b-66d1130b4b57,5), (fd1471b0-1b95-465f-95c7-256ec3e65e26,5), (fd6b98aa-6662-4d30-b63d-683264bf4a3d,5), (285278ad-affb-4b17-a442-3b83dae12576,5)))
     * (17,List((6b53902b-93e2-4e08-be23-da9e12699dc5,7), (39b5df21-1c7f-4a8a-9766-84ee5f6954e0,6), (f632bbe4-e5ba-4dcf-91ef-89f9688376a9,6), (b21551c7-40e1-405c-81e2-d7caf53e6c07,5), (6501d135-cf17-4020-9852-51b6c3d5f1d7,5), (36003baf-6d64-49be-a047-9fc8c0e76ca8,5), (595f8ca5-7d03-4a32-956e-894821cd17f9,5), (8828d848-74b6-4bea-9a12-4c4d9f6e20c8,5), (5a841cd1-99ed-47fa-af9b-72183fd37b67,5), (7328ce83-2c98-4d39-a1bf-1af472bb8c68,5)))
     * (13,List((d4125ad2-454d-40b6-b0d6-769ff58a26b0,6), (23087483-70c1-4e5f-9d53-5f7020032929,6), (0031c47d-1bcf-43d9-ad6a-252339b47082,5), (a550b6e4-6e9f-45f8-86ec-040919295efd,5), (7a20dbce-58a5-464b-86e6-7abef1cedd64,5), (8b9a0f5a-bff9-4f37-b3bb-6aa3c24d24f7,5), (0f0c33f2-a9c9-4264-befe-dce0d892526a,5), (70e2e082-08e6-4e14-bec6-6ac266e3bf90,5), (fe75781f-0aa1-4cfd-a11f-128ee35707db,5), (9584d531-47b6-455c-9843-24e45f47939b,5)))
     *
     */

    sc.stop()
  &amp;#125;
  def top10Category(dataRdd:RDD[String]) = &amp;#123;
    val flatRdd: RDD[(String, (Int, Int, Int))] = dataRdd.flatMap(data =&amp;gt; &amp;#123;
      val cols = data.split(&amp;quot;,&amp;quot;) //按照列拍扁
      if (cols(6) != &amp;quot;-1&amp;quot; &amp;amp;&amp;amp; cols(7) != &amp;quot;-1&amp;quot;) &amp;#123;
        List((cols(6), (1, 0, 0))) //统计品类的点击数量,点击的品类ID和产品ID不为-1，表示点击数据
      &amp;#125; else if (cols(8) != &amp;quot;null&amp;quot; &amp;amp;&amp;amp; cols(9) != &amp;quot;null&amp;quot;) &amp;#123;
        cols(8).split(&amp;quot;-&amp;quot;).map(category_id =&amp;gt; (category_id, (0, 1, 0))) //品类ID &amp;quot;1-2-3&amp;quot;
      &amp;#125; else if (cols(10) != &amp;quot;null&amp;quot; &amp;amp;&amp;amp; cols(11) != &amp;quot;null&amp;quot;) &amp;#123;
        cols(10).split(&amp;quot;-&amp;quot;).map(category_id =&amp;gt; (category_id, (0, 0, 1))) //品类ID &amp;quot;1-2-3&amp;quot;
      &amp;#125; else &amp;#123;
        Nil
      &amp;#125;
    &amp;#125;)
    val resRdd: RDD[(String, (Int, Int, Int))] = flatRdd.reduceByKey((a, b) =&amp;gt; (a._1 + b._1, a._2 + b._2, a._3 + b._3)) //    ( 品类ID，( 点击数量, 下单数量, 支付数量 ) )
    resRdd.take(10).map(_._1)//只需要前10的品类ID
  &amp;#125;

&amp;#125;

&lt;/code&gt;&lt;/pre&gt;
</content>
        <category term="Spark" />
        <updated>2021-10-22T06:24:49.000Z</updated>
    </entry>
    <entry>
        <id>http://spark.kanseaveg.xyz/2021/10/22/SparkCore-ACC%E7%B4%AF%E5%8A%A0%E5%99%A8/</id>
        <title>SparkCore-ACC累加器</title>
        <link rel="alternate" href="http://spark.kanseaveg.xyz/2021/10/22/SparkCore-ACC%E7%B4%AF%E5%8A%A0%E5%99%A8/"/>
        <content type="html">&lt;h3 id=&#34;sparkcore-acc累加器&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#sparkcore-acc累加器&#34;&gt;#&lt;/a&gt; SparkCore-ACC 累加器&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;package com.kanseaveg.rdd

import org.apache.spark.rdd.RDD
import org.apache.spark.util.&amp;#123;AccumulatorV2, LongAccumulator&amp;#125;
import org.apache.spark.&amp;#123;SparkConf, SparkContext&amp;#125;

import scala.collection.mutable

/**
 * 累加器ACC：
 * [@href=&amp;quot;https://raw.githubusercontent.com/kanseavegs/picgo_images/master/data/20211015140254.png&amp;quot;]
 */
object ACC &amp;#123;
  def main(args: Array[String]): Unit = &amp;#123;

    val conf = new SparkConf().setMaster(&amp;quot;local[*]&amp;quot;).setAppName(&amp;quot;AdsRDDdemo&amp;quot;)
    val sc = new SparkContext(conf)
    val rdd: RDD[Int] = sc.makeRDD(List(1, 2, 3, 4), 2)
    //全部分区数据求和
//    println(rdd.reduce(_ + _)) //reduce包括分区内的计算和分区间的计算
//    var sum = 0
//    rdd.foreach( num =&amp;gt; sum+=num) // [@href=&amp;quot;https://raw.githubusercontent.com/kanseavegs/picgo_images/master/data/20211015141205.png&amp;quot;]
//    println(sum)  // sum竟然等于0！竟然等于0！竟然等于0！ 因为此时数据分发到各个Executor上运行(1+2=3),(3+4=7)，但是并没有返回汇总,Driver段sum依旧是0

    //获取系统的累加器：Spark默认就提供了简单数据聚合的累加器
    val sumAcc: LongAccumulator = sc.longAccumulator(&amp;quot;sum&amp;quot;) //分布式共享只写变量
    rdd.foreach(
      num =&amp;gt; sumAcc.add(num)
    )
//    println(sumAcc.value)

    /**
     * 自定义累加器: 实现wordcount shuffle前提前将数据和汇总
     */
    val words: RDD[String] = sc.makeRDD(List(&amp;quot;hello scala&amp;quot;, &amp;quot;hello spark&amp;quot;))
    val wcAcc: MyAccumulator = new MyAccumulator() //创建累加器对象
    sc.register(wcAcc,&amp;quot;wordCountACC&amp;quot;) //向spark上下文进行注册
    words.foreach(word =&amp;gt; wcAcc.add(word)) //使用累加器对数据进行累加
    println(wcAcc.value) //Map(hello spark -&amp;gt; 1, hello scala -&amp;gt; 1)

    sc.stop()
  &amp;#125;

  /**
   * 自定义累加器 1.继承AccumulatorV2类并传入输入输出类型
   *            2.实现方法
   */
  class MyAccumulator extends AccumulatorV2[String,mutable.Map[String,Long]] &amp;#123;

    private var wcMap = mutable.Map[String,Long]()

    //判断是否为初始状态
    override def isZero: Boolean = wcMap.isEmpty

    //复制累加器
    override def copy(): AccumulatorV2[String, mutable.Map[String, Long]] = new MyAccumulator

    override def reset(): Unit = wcMap.clear()

    //获取累加器需要计算的值
    override def add(word: String): Unit = &amp;#123;
        wcMap.update(word, wcMap.getOrElse(word,0L) + 1)
    &amp;#125;

    //Driver合并各个分区多个累加器
    override def merge(other: AccumulatorV2[String, mutable.Map[String, Long]]): Unit = &amp;#123; //other 表示另一分区的累加器
      val curAccMap: mutable.Map[String, Long] = this.wcMap
      val otherAccMap: mutable.Map[String, Long] = other.value
      otherAccMap.foreach( tp =&amp;gt;  curAccMap.update(tp._1,curAccMap.getOrElse(tp._1,0L) + tp._2)) //把tuple作为整体传入 _1为word,_2为count
//      otherAccMap.foreach&amp;#123; case (word,count) =&amp;gt; curAccMap.update(word, curAccMap.getOrElse(word,0L)+count)&amp;#125; //老师写法
    &amp;#125;
    //累加器返回结果
    override def value: mutable.Map[String, Long] = &amp;#123;
      wcMap
    &amp;#125;
  &amp;#125;
&amp;#125;
&lt;/code&gt;&lt;/pre&gt;
</content>
        <category term="Spark" />
        <updated>2021-10-22T06:07:10.000Z</updated>
    </entry>
    <entry>
        <id>http://spark.kanseaveg.xyz/2021/10/22/SparkCore-BroadcastVariable%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/</id>
        <title>SparkCore-BroadcastVariable广播变量</title>
        <link rel="alternate" href="http://spark.kanseaveg.xyz/2021/10/22/SparkCore-BroadcastVariable%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/"/>
        <content type="html">&lt;h3 id=&#34;sparkcore-broadcastvariable广播变量&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#sparkcore-broadcastvariable广播变量&#34;&gt;#&lt;/a&gt; SparkCore-BroadcastVariable 广播变量&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;package com.kanseaveg.rdd

import org.apache.spark.broadcast.Broadcast
import org.apache.spark.rdd.RDD
import org.apache.spark.&amp;#123;SparkConf, SparkContext&amp;#125;

/**
 *                                      【广播变量】
 *    [@href=&amp;quot;https://www.bbsmax.com/A/QW5Yv9rMdm/&amp;quot;] 
 *    共享变量出现的原因：通常在向 Spark 传递函数时，比如使用map或reduce传条件或变量时，在driver端定义变量，但是集群中运行的每个任务都会得到这些变量的一份
 *                     新的副本，更新这些副本的值driver端的对应变量并不会随之更新。Spark 的两个共享变量，广播变量与累加器分别为变量提供广播与聚合功能，突破了变量不能共享的限制。
 *    Spark两种共享变量：广播变量（broadcast variable）与累加器（accumulator），广播变量常用来高效分发较大的对象，而累加器用来对信息进行聚合。
 *
 *                                      【广播变量 vs 累加器】
 *    累加器就是只写变量 通常就是做事件统计用的 因为rdd是在不同的excutor去执行的 你在不同excutor中累加的结果 没办法汇总到一起 这个时候就需要累加器来帮忙完成
 *    广播变量是只读变量 正常的话我们在driver定义一个变量 需要序列化 才能在excutor端使用  而且是每个task都需要传输一次 这样如果我们定义的对象很大的话就会产生大量的IO
 *                     如果你把这个大对象定义成广播变量的话 我们只需要每个excutor发送一份就可以 如果task需要时 只需要从excutor拉取就可以了。可以减轻集群driver和executor间的通信压力，节省集群资源。
 *
 * 为什么要使用这个广播变量： 如果我们要在分布式计算里面分发大对象，例如：字典，集合，黑白名单等，这个都会由Driver端进行分发，
 *                        一般来讲，如果这个变量不是广播变量，那么每个task就会分发一份，这在task数目十分多的情况下Driver的带宽会成为系统的瓶颈，
 *                        而且会大量消耗task服务器上的资源，如果将这个变量声明为广播变量，那么知识每个executor拥有一份，这个executor启动的task
 *                        会共享这个变量，节省了通信的成本和服务器的资源。
 */
object BroadCastVariable &amp;#123;
  def main(args: Array[String]): Unit = &amp;#123;
    val conf = new SparkConf().setMaster(&amp;quot;local[*]&amp;quot;).setAppName(&amp;quot;aaaa&amp;quot;)
    val sc = new SparkContext(conf)

    /**
     * sample: 过滤来访ip
     */
    val lineRdd = sc.textFile(&amp;quot;datas/ip_list.txt&amp;quot;);

    //使用广播变量时，driver第一次向executor发送task时候，发送blackList，缓存到blockmanager，以后不会再发送
    //在driver端进行广播blackList
    val blackList = List[String](&amp;quot;8.8.8.8&amp;quot;,&amp;quot;114.114.114.114&amp;quot;);//定义黑名单ip
    val broadCast: Broadcast[List[String]] = sc.broadcast(blackList)

    //在executor端用broadCast.value获取blackList的值
    val filterRdd: RDD[String] = lineRdd.filter(ip =&amp;gt; !broadCast.value.contains(ip))
    filterRdd.foreach(println)

    sc.stop()
  &amp;#125;
&amp;#125;
&lt;/code&gt;&lt;/pre&gt;
</content>
        <category term="Spark" />
        <updated>2021-10-22T06:07:10.000Z</updated>
    </entry>
    <entry>
        <id>http://spark.kanseaveg.xyz/2021/10/22/SparkCore-RDD%E5%8D%95%E5%80%BC%E7%B1%BB%E5%9E%8B/</id>
        <title>SparkCore-RDD转换算子单值类型</title>
        <link rel="alternate" href="http://spark.kanseaveg.xyz/2021/10/22/SparkCore-RDD%E5%8D%95%E5%80%BC%E7%B1%BB%E5%9E%8B/"/>
        <content type="html">&lt;h3 id=&#34;sparkcore-rdd转换算子单值类型&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#sparkcore-rdd转换算子单值类型&#34;&gt;#&lt;/a&gt; SparkCore-RDD 转换算子单值类型&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;package com.kanseaveg.rdd

import org.apache.spark.rdd.RDD
import org.apache.spark.&amp;#123;SparkConf, SparkContext&amp;#125;

/**
 * 单值转换算子:
 *
 * 分区内数据的执行是有序的
 */
object SingleValueRDD &amp;#123;
  def main(args: Array[String]): Unit = &amp;#123;
    val conf = new SparkConf().setMaster(&amp;quot;local[*]&amp;quot;).setAppName(&amp;quot;SingleValueRDD&amp;quot;)
    val sc = new SparkContext(conf)
    val baseRdd: RDD[Int] = sc.makeRDD(List(1, 2, 3, 4),2)
    val baseListRdd: RDD[List[Int]] = sc.makeRDD(List(
      List(1, 2), List(3, 4)
    ))

    /**
     * map：逐条转换，串行操作
     */
    val mapRdd: RDD[Int] = baseRdd.map(_ * 2)
//    mapRdd.collect().foreach(println)

    /**
     * mapPartitions: 设置缓冲，一个分区的数据进行转换，但是需要传入一个迭代器 传回一个迭代器，批处理
     * 但是会将整个分区加载到内存进行引用，如果处理完的数据没被释放掉，则会存在对象的引用
     */
    val mapPartitionsRdd: RDD[Int] = baseRdd.mapPartitions(iter =&amp;gt; List(iter.max).iterator) //取得分区最大值，返回迭代器
//    mapPartitionsRdd.collect().foreach(println)

    /**
     * mapPartitionsIndex: 在mappartitions基础上增加了分区索引
     */
//    val mapPartitionsIndexRdd: RDD[Int] = baseRdd.mapPartitionsWithIndex( (index,iter) =&amp;gt; &amp;#123; if(index==1) iter else Nil.iterator&amp;#125;); //取出分区1中的数据
    val mapPartitionsIndexRdd: RDD[(Int,Int)] = baseRdd.mapPartitionsWithIndex( (index,iter) =&amp;gt; iter.map( num =&amp;gt; &amp;#123;(index,num)&amp;#125;)); //取出分区号和分区中的数据
//    mapPartitionsIndexRdd.foreach(println)

    /**
     * flatMap: 扁平映射
     */
    val flatmapRdd: RDD[Int] = baseListRdd.flatMap(list =&amp;gt; list) //前面的list表示各个List元素，后面那个list表示封装到一个新的list中
//    flatmapRdd.collect.foreach(println)

    /**
     * glom:将同一个分区的数据直接转换为相同类型的内存数组进行处理，分区不变
     *    原来：val baseRdd: RDD[Int] = sc.makeRDD(List(1, 2, 3, 4),2)  数据分布在两个分区中 【拆散】至同一个分区中
     *    现在：val glomRdd: RDD[Array[Int]] ，同一个分区的数据【合并】至一个数组中
     *    res:  1,2
     *          3,4
     */
    val glomRdd: RDD[Array[Int]] = baseRdd.glom()
//    glomRdd.collect.foreach(data=&amp;gt; println(data.mkString(&amp;quot;,&amp;quot;)))

    /**
     * groupBy:将数据根据指定的规则进行分组, [分区默认不变]，但是数据会被打乱重新组合[shuffle]
     *        参数：传入一个分组函数，根据返回的分组key进行分组，相同的key值会放入到一个组中
     *        注意：分组和分区没有必然的关系
     * (0,CompactBuffer(2, 4))
     * (1,CompactBuffer(1, 3))
      */
    val groupByRdd: RDD[(Int, Iterable[Int])] = baseRdd.groupBy(_ % 2)  //模2相同的放在一个组中
//    groupByRdd.collect.foreach(println)

    /**
     * fliter: 过滤
     *  当数据进行筛选过滤后，分区不变，但是可能会出现数据倾斜问题
     */
    val fliterRdd: RDD[Int] = baseRdd.filter(_%2==0)
//    fliterRdd.collect.foreach(println)

    /**
     * sample: 样本抽取
     *        withReplacement:  抽取数据放回/不放回
     *        fraction: 抽取的几率，范围在[0,1]之间,0：全不取；1：全取,每条数据预期被抽取到的期望概率,基准值
     *        seed: Long = Utils.random.nextLong): RDD[T] 随机方法的随机数种子
     */
    val sampleRdd: RDD[Int] = baseRdd.sample(false, 0.4, 1)
//    sampleRdd.collect.foreach(println)

    /**
     * distinct: 去重
     *  scala中用hashset去重，spark中
     *  case _ =&amp;gt; map(x =&amp;gt; (x, null)).reduceByKey((x, _) =&amp;gt; x, numPartitions).map(_._1) 相同的key做聚合
     */
    val distinctRdd: RDD[Int] = baseRdd.distinct()
//    distinctRdd.collect.foreach(println)


    /**
     * coalesce: 缩减分区, 收缩合并分区，减少分区的个数，减小任务调度成本， 用于大数据集过滤后，提高小数据集的执行效率
     *    默认情况下不会将分区的数据进行打乱重新组合[没有shuffle]，会出现数据倾斜问题
     *    第二个参数：是否进行shuffle操作
     *    coalesce也可以扩大分区，不过需要传入第二个参数shuffle=true
     */
    val coalesceRdd: RDD[Int] = baseRdd.coalesce(1,true) //2个分区缩减为1个分区
//    coalesceRdd.collect.foreach(println)

    /**
     * repartition: 扩大分区,内部还是调用 coalesce 操作 参数 shuffle 的默认值为 true
     */
    val repartitionRdd: RDD[Int] = baseRdd.repartition(4)
//    repartitionRdd.collect.foreach(println)

    /**
     * sortBy: 排序，排序后分区数不变，默认为升序排列, 中间存在 shuffle 的过程【打乱数据】
     */
    val sortByRdd: RDD[Int] = baseRdd.sortBy(num =&amp;gt; num)
//    sortByRdd.collect.foreach(println)

  &amp;#125;
&amp;#125;

&lt;/code&gt;&lt;/pre&gt;
</content>
        <category term="Spark" />
        <updated>2021-10-22T06:07:10.000Z</updated>
    </entry>
    <entry>
        <id>http://spark.kanseaveg.xyz/2021/10/22/SparkCore-RDD%E5%8F%8C%E5%80%BC%E7%B1%BB%E5%9E%8B/</id>
        <title>SparkCore-RDD转换算子双值类型</title>
        <link rel="alternate" href="http://spark.kanseaveg.xyz/2021/10/22/SparkCore-RDD%E5%8F%8C%E5%80%BC%E7%B1%BB%E5%9E%8B/"/>
        <content type="html">&lt;h3 id=&#34;sparkcore-rdd转换算子-双值类型&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#sparkcore-rdd转换算子-双值类型&#34;&gt;#&lt;/a&gt; SparkCore-RDD 转换算子 - 双值类型&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;package com.kanseaveg.rdd

import org.apache.spark.&amp;#123;SparkConf, SparkContext&amp;#125;
import org.apache.spark.rdd.RDD

/**
 * 双值转换算子:  两个数据源之间的关联操作
 */
object DoubleValueRDD &amp;#123;
  def main(args: Array[String]): Unit = &amp;#123;
    val conf = new SparkConf().setMaster(&amp;quot;local[*]&amp;quot;).setAppName(&amp;quot;SingleValueRDD&amp;quot;)
    val sc = new SparkContext(conf)
    val rdd1: RDD[Int] = sc.makeRDD(List(1, 2, 3, 4))
    val rdd2: RDD[Int] = sc.makeRDD(List(3, 4, 5, 6))
    //交集
    val intersectionRdd: RDD[Int] = rdd1.intersection(rdd2)
    println(intersectionRdd.collect().mkString(&amp;quot;,&amp;quot;)) //3,4
    //并集
    val unionRdd: RDD[Int] = rdd1.union(rdd2)
    println(unionRdd.collect().mkString(&amp;quot;,&amp;quot;))  //1,2,3,4,3,4,5,6
    //差集
    val subtractRdd: RDD[Int] = rdd1.subtract(rdd2)
    println(subtractRdd.collect().mkString(&amp;quot;,&amp;quot;))  //1,2
    //拉链
    val zipRdd: RDD[(Int, Int)] = rdd1.zip(rdd2)
    println(zipRdd.collect().mkString(&amp;quot;,&amp;quot;)) //(1,3),(2,4),(3,5),(4,6)
    
  &amp;#125;
&amp;#125;
&lt;/code&gt;&lt;/pre&gt;
</content>
        <category term="Spark" />
        <updated>2021-10-22T06:07:10.000Z</updated>
    </entry>
    <entry>
        <id>http://spark.kanseaveg.xyz/2021/10/22/SparkCore-RDD%E6%8C%81%E4%B9%85%E5%8C%96/</id>
        <title>SparkCore-RDD持久化</title>
        <link rel="alternate" href="http://spark.kanseaveg.xyz/2021/10/22/SparkCore-RDD%E6%8C%81%E4%B9%85%E5%8C%96/"/>
        <content type="html">&lt;h3 id=&#34;sparkcore-rdd持久化&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#sparkcore-rdd持久化&#34;&gt;#&lt;/a&gt; SparkCore-RDD 持久化&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;package com.kanseaveg.rdd

import org.apache.spark.rdd.RDD
import org.apache.spark.&amp;#123;SparkConf, SparkContext&amp;#125;

/**
 * 持久化RDD : [@href=&amp;quot;https://raw.githubusercontent.com/kanseavegs/picgo_images/master/data/20211015105854.png&amp;quot;]
 */
object PersistRDD &amp;#123;
  def main(args: Array[String]): Unit = &amp;#123;

    val conf = new SparkConf().setMaster(&amp;quot;local[*]&amp;quot;).setAppName(&amp;quot;AdsRDDdemo&amp;quot;)
    val sc = new SparkContext(conf)
    val baseRdd: RDD[String] = sc.makeRDD(List(&amp;quot;Hello Scala&amp;quot;, &amp;quot;Hello Spark&amp;quot;))

    /**
     * 未持久化： 由于RDD不存储数据，如果一个RDD需要重复使用，那么需要从头再次执行来获取数据
     *            RDD对象是可以重用的，但是数据无法重用
     */
    val mapRdd: RDD[(String, Int)] = baseRdd.flatMap(_.split(&amp;quot; &amp;quot;)).map((_, 1))
//    println(&amp;quot;********复用中间变量********&amp;quot;)
//    mapRdd.reduceByKey(_+_).collect().foreach(println)
//    mapRdd.groupByKey().collect().foreach(println)

    /**
     * cache持久化操作，仍调用persist，默认采用保存至内存中的策略
     */
//    println(&amp;quot;********通过 Cache 或者 Persist 方法将前面的计算结果缓存********&amp;quot;)
//    mapRdd.cache()

    /**
     * presist持久化操作，可以设定持久化策略，保存在内存中还是磁盘中 StorageLevel
     */
//    mapRdd.persist(StorageLevel.DISK_ONLY)

    /**
     * 持久化操作会在行动算子执行时才会触发
     */
//    mapRdd.reduceByKey(_+_).collect().foreach(println)
//    mapRdd.groupByKey().collect().foreach(println)

    /**
     * checkpoint检查点持久化操作，需要落盘指定检查点路径
     *          检查点路径中保存的文件，在作业执行完后 不会被删除
     */
//    sc.setCheckpointDir(&amp;quot;cp&amp;quot;)
//    mapRdd.checkpoint()

    /**
     * cache vs persist vs checkpoint
     * cache： 临时存储数据在内存，会在血缘关系中添加新的依赖
     * persist： ~在磁盘，涉及磁盘IO，性能低但数据安全，作业执行完毕，数据文件会被自动丢弃
     * checkpoint: ~在磁盘，同样涉及磁盘IO，一般情况下联合cache一齐使用，会独立执行作业
     */




  &amp;#125;
&amp;#125;
&lt;/code&gt;&lt;/pre&gt;
</content>
        <category term="Spark" />
        <updated>2021-10-22T06:07:10.000Z</updated>
    </entry>
    <entry>
        <id>http://spark.kanseaveg.xyz/2021/10/22/SparkCore-RDD%E8%A1%80%E7%BC%98%E5%85%B3%E7%B3%BB/</id>
        <title>SparkCore-RDD转换算子-双值类型</title>
        <link rel="alternate" href="http://spark.kanseaveg.xyz/2021/10/22/SparkCore-RDD%E8%A1%80%E7%BC%98%E5%85%B3%E7%B3%BB/"/>
        <content type="html">&lt;h3 id=&#34;sparkcore-rdd之间的血缘关系&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#sparkcore-rdd之间的血缘关系&#34;&gt;#&lt;/a&gt; SparkCore-RDD 之间的血缘关系&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;
package com.kanseaveg.rdd

import org.apache.spark.rdd.RDD
import org.apache.spark.&amp;#123;SparkConf, SparkContext&amp;#125;

/**
 * 血缘关系： [@href=&amp;quot;https://raw.githubusercontent.com/kanseavegs/picgo_images/master/data/20211014154027.png&amp;quot;]
 *
 * ******************************textFile************************
 * (2) datas/word.txt MapPartitionsRDD[1] at textFile at DependencyRDD.scala:15 []              current
 * |  datas/word.txt HadoopRDD[0] at textFile at DependencyRDD.scala:15 []
 * =============================flatmap==========================
 * (2) MapPartitionsRDD[2] at flatMap at DependencyRDD.scala:18 []
 * |  datas/word.txt MapPartitionsRDD[1] at textFile at DependencyRDD.scala:15 []              current
 * |  datas/word.txt HadoopRDD[0] at textFile at DependencyRDD.scala:15 []
 * ******************************map************************
 * (2) MapPartitionsRDD[3] at map at DependencyRDD.scala:21 []
 * |  MapPartitionsRDD[2] at flatMap at DependencyRDD.scala:18 []
 * |  datas/word.txt MapPartitionsRDD[1] at textFile at DependencyRDD.scala:15 []              current
 * |  datas/word.txt HadoopRDD[0] at textFile at DependencyRDD.scala:15 []
 * ==========================reduceByKey==========================
 * (2) ShuffledRDD[4] at reduceByKey at DependencyRDD.scala:24 []              current, but +- represents the shuffle process.
 * +-(2) MapPartitionsRDD[3] at map at DependencyRDD.scala:21 []
 * |  MapPartitionsRDD[2] at flatMap at DependencyRDD.scala:18 []
 * |  datas/word.txt MapPartitionsRDD[1] at textFile at DependencyRDD.scala:15 []
 * |  datas/word.txt HadoopRDD[0] at textFile at DependencyRDD.scala:15 []
 * (Hello,2)
 * (Scala,1)
 * (Spark,1)
 *
 * Process finished with exit code 0
 *
 *
 * oneToOne依赖： 新的RDD的一个分区的数据依赖于旧RDD【一个分区】的数据  https://raw.githubusercontent.com/kanseavegs/picgo_images/master/data/20211014155819.png
 *
 *                @DeveloperApi
 *                class OneToOneDependency[T](rdd: RDD[T]) extends NarrowDependency[T](rdd) &amp;#123;  //继承自窄依赖
 *                窄依赖表示每一个父(上游)RDD 的 Partition 最多被子（下游）RDD 的一个 Partition 使用，
 *                窄依赖我们形象的比喻为独生子女
 * oneToMany依赖：新的RDD的一个分区的数据依赖于旧RDD【多个分区】的数据  https://raw.githubusercontent.com/kanseavegs/picgo_images/master/data/20211014155913.png
 *                class ShuffleDependency[K: ClassTag, V: ClassTag, C: ClassTag](  extends Dependency[Product2[K, V]] &amp;#123;//由于窄依赖存在 故相对应的称为宽依赖
 *                宽依赖表示同一个父（上游）RDD 的 Partition 被多个子（下游）RDD 的 Partition 依赖，会引起 Shuffle，总结：宽依赖我们形象的比喻为多生。
 *
 *
 */


/**
 *【RDD 阶段划分】：DAG（Directed Acyclic Graph）有向无环图是由点和线组成的拓扑图形，该图形具有方向，不会闭环。例如，DAG 记录了 RDD 的转换过程和任务的阶段。
 *                Shuffle过程会产生阶段划分（因为要等待来自不同RDD的数据），oneToOne整个就是一个阶段
 *【RDD任务划分】：待定。。 P96-98
 */



object DependencyRDD &amp;#123;
  def main(args: Array[String]): Unit = &amp;#123;

    val conf = new SparkConf().setMaster(&amp;quot;local[*]&amp;quot;).setAppName(&amp;quot;AdsRDDdemo&amp;quot;)
    val sc = new SparkContext(conf)
    println(&amp;quot;******************************textFile************************&amp;quot;)
    val lines: RDD[String] = sc.textFile(&amp;quot;datas/word.txt&amp;quot;)
    println(lines.toDebugString)
    println(&amp;quot;=============================flatmap==========================&amp;quot;)
    val words: RDD[String] = lines.flatMap(_.split(&amp;quot; &amp;quot;))
    println(words.toDebugString)
    println(&amp;quot;******************************map************************&amp;quot;)
    val wordToOne: RDD[(String, Int)] = words.map(word =&amp;gt; (word, 1))
    println(wordToOne.toDebugString)
    println(&amp;quot;==========================reduceByKey==========================&amp;quot;)
    val wordToSum: RDD[(String, Int)] = wordToOne.reduceByKey(_ + _)
    println(wordToSum.toDebugString)
    wordToSum.collect().foreach(println)
    sc.stop()
  &amp;#125;

&amp;#125;

&lt;/code&gt;&lt;/pre&gt;
</content>
        <category term="Spark" />
        <updated>2021-10-22T06:07:10.000Z</updated>
    </entry>
    <entry>
        <id>http://spark.kanseaveg.xyz/2021/10/22/SparkCore-RDD%E8%A1%8C%E5%8A%A8%E7%AE%97%E5%AD%90/</id>
        <title>SparkCore-RDD行动算子</title>
        <link rel="alternate" href="http://spark.kanseaveg.xyz/2021/10/22/SparkCore-RDD%E8%A1%8C%E5%8A%A8%E7%AE%97%E5%AD%90/"/>
        <content type="html">&lt;h3 id=&#34;sparkcore-rdd行动算子&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#sparkcore-rdd行动算子&#34;&gt;#&lt;/a&gt; SparkCore-RDD 行动算子&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;package com.kanseaveg.rdd

import org.apache.spark.rdd.RDD
import org.apache.spark.&amp;#123;SparkConf, SparkContext&amp;#125;

/**
 * 行动算子：触发作业执行
 *      底层实际调用的是 sc.runJob(this, (iter: Iterator[T]) =&amp;gt; iter.toArray)
 *       =&amp;gt; dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)
 *       =&amp;gt; val waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties)
 *       =&amp;gt; eventProcessLoop.post(JobSubmitted(jobId, rdd, func2, partitions.toArray, callSite, waiter ....
 *       =&amp;gt; private[scheduler] def handleJobSubmitted(jobId: Int,....
 *       =&amp;gt; val job = new ActiveJob(jobId, finalStage, callSite, listener, properties) 创建activeJob 并提交执行
 *
 */
object ActionRDD &amp;#123;
  def main(args: Array[String]): Unit = &amp;#123;

    val conf = new SparkConf().setMaster(&amp;quot;local[*]&amp;quot;).setAppName(&amp;quot;AdsRDDdemo&amp;quot;)
    val sc = new SparkContext(conf)
    val baseRdd: RDD[Int] = sc.makeRDD(List(1, 2, 3, 4),2)

    /**
     * reduce 聚集 RDD 中的所有元素
     */
//    println(baseRdd.reduce(_ + _)) //10

    /**
     * collect: 将不同分区的数据按照分区顺序采集到Driver端内存中，形成数组
     */
//    println(baseRdd.collect().mkString(&amp;quot;,&amp;quot;)) //1,2,3,4

    /**
     * count: 统计数据源中数据的个数
     * first： 获取数据源中数据的第一个
     * take: 获取数据源中N个数据
     * takeOrdered: 获取数据源中排序后的N个数据
     */
//    println(baseRdd.count()) //4
//    println(baseRdd.first()) //1
//    println(baseRdd.take(3).mkString(&amp;quot;,&amp;quot;)) //1,2,3
//    println(baseRdd.takeOrdered(3).mkString(&amp;quot;,&amp;quot;)) //1,2,3

    /**
     * aggregate: 分区的数据通过初始值和分区内的数据进行聚合，然后再和初始值进行分区间的数据聚合
     *      aggregateByKey vs aggregate
     *      aggregateByKey: 初始值只会参与分区内计算  (10+3) + (10+7) = 30
     *      aggregate： 初始值会参与分区内和分区间的计算 (10+3) + (10+7) + 10 = 40
     * fold: aggregate简化版：aggregate分区内和分区间规则相同时，进行简化
     */
//    println(baseRdd.aggregate(10)(_ + _, _ + _)) //40
//    println(baseRdd.fold(10)(_ + _))  //40

    /**
     * countByKey: 统计每种 key 的个数
     * countByValue: 统计每种 value 的个数
     */
//    println(baseRdd.countByValue()) // Map(4 -&amp;gt; 1, 2 -&amp;gt; 1, 1 -&amp;gt; 1, 3 -&amp;gt; 1)
//    println(sc.makeRDD(List((&amp;quot;a&amp;quot;, 1), (&amp;quot;a&amp;quot;, 20), (&amp;quot;a&amp;quot;, 10), (&amp;quot;b&amp;quot;, 66))).countByKey())  //Map(a -&amp;gt; 3, b -&amp;gt; 1)

    /**
     * 小小wordcount Sample: 8个实现wordcount方法
     * (Hello,2)
     * (Spark,1)
     * (Scala,1)
     */
//    sc.makeRDD(List(&amp;quot;Hello Scala&amp;quot;,&amp;quot;Hello Spark&amp;quot;)).flatMap(_.split(&amp;quot; &amp;quot;)).groupBy(word=&amp;gt;word).mapValues(_.size).collect().foreach(println)
//    sc.makeRDD(List(&amp;quot;Hello Scala&amp;quot;,&amp;quot;Hello Spark&amp;quot;)).flatMap(_.split(&amp;quot; &amp;quot;)).map((_,1)).groupByKey().mapValues(_.size).collect().foreach(println) //存在shuffle 效率不高
//    sc.makeRDD(List(&amp;quot;Hello Scala&amp;quot;,&amp;quot;Hello Spark&amp;quot;)).flatMap(_.split(&amp;quot; &amp;quot;)).map((_,1)).reduceByKey(_+_).collect().foreach(println)
//    sc.makeRDD(List(&amp;quot;Hello Scala&amp;quot;,&amp;quot;Hello Spark&amp;quot;)).flatMap(_.split(&amp;quot; &amp;quot;)).map((_,1)).aggregateByKey(0)(_+_,_+_).collect().foreach(println)
//    sc.makeRDD(List(&amp;quot;Hello Scala&amp;quot;,&amp;quot;Hello Spark&amp;quot;)).flatMap(_.split(&amp;quot; &amp;quot;)).map((_,1)).foldByKey(0)(_+_).collect().foreach(println)
//    sc.makeRDD(List(&amp;quot;Hello Scala&amp;quot;,&amp;quot;Hello Spark&amp;quot;)).flatMap(_.split(&amp;quot; &amp;quot;)).map((_,1)).combineByKey(v=&amp;gt;v,(x:Int,y:Int)=&amp;gt;x+y,(x:Int,y:Int)=&amp;gt;x+y).collect().foreach(println)
//    sc.makeRDD(List(&amp;quot;Hello Scala&amp;quot;,&amp;quot;Hello Spark&amp;quot;)).flatMap(_.split(&amp;quot; &amp;quot;)).map((_,1)).countByKey().foreach(println)
//    sc.makeRDD(List(&amp;quot;Hello Scala&amp;quot;,&amp;quot;Hello Spark&amp;quot;)).flatMap(_.split(&amp;quot; &amp;quot;)).countByValue().foreach(println)


    /**
     * save: 保存文件
     * def saveAsTextFile(path: String): Unit
     * def saveAsObjectFile(path: String): Unit
     * def saveAsSequenceFile : //SparkCore-RDD行动算子规定数据的格式必须是键值类型
     */
//    baseRdd.saveAsTextFile(&amp;quot;output&amp;quot;)
//    baseRdd.saveAsObjectFile(&amp;quot;output&amp;quot;)
//    baseRdd.map((_,1)).saveAsSequenceFile(&amp;quot;output&amp;quot;)


    /**
     * foreach
     */
//    println(&amp;quot;在Driver端内存中执行打印操作&amp;quot;) //https://raw.githubusercontent.com/kanseavegs/picgo_images/master/data/20211013163016.png
//    baseRdd.collect().foreach(println) //collect进行了分区顺序汇总，打印有序
//    println(&amp;quot;在Executor端执行打印操作&amp;quot;)  //
//    baseRdd.foreach(println) //分区直接打印，无序

    /**
     * PS：算子：Operator（操作）
     *      RDD的方法和Scala集合对象的方法不一样哈
     *      集合对象的方法都是在同一个节点的内存中完成的
     *      RDD方法可以将计算逻辑发送到Executor端（分布式节点）执行
     *      为了区分不同的处理效果，所以才将RDD的方法称为算子
     *      RDD的方法外部操作都是在Driver端执行的，而方法内部的逻辑代码都是在Executor端执行的。
     */


  &amp;#125;
&amp;#125;

&lt;/code&gt;&lt;/pre&gt;
</content>
        <category term="Spark" />
        <updated>2021-10-22T06:07:10.000Z</updated>
    </entry>
    <entry>
        <id>http://spark.kanseaveg.xyz/2021/10/22/SparkCore-RDD%E9%94%AE%E5%80%BC%E7%B1%BB%E5%9E%8B/</id>
        <title>SparkCore-RDD转换算子键值类型</title>
        <link rel="alternate" href="http://spark.kanseaveg.xyz/2021/10/22/SparkCore-RDD%E9%94%AE%E5%80%BC%E7%B1%BB%E5%9E%8B/"/>
        <content type="html">&lt;h3 id=&#34;sparkcore-rdd转换算子键值类型&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#sparkcore-rdd转换算子键值类型&#34;&gt;#&lt;/a&gt; SparkCore-RDD 转换算子键值类型&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;package com.kanseaveg.rdd

import org.apache.spark.rdd.RDD
import org.apache.spark.&amp;#123;SparkConf, SparkContext&amp;#125;

/**
 * 键值类型RDD
 */
object KeyValueRDD &amp;#123;
  def main(args: Array[String]): Unit = &amp;#123;
    val conf = new SparkConf().setMaster(&amp;quot;local[*]&amp;quot;).setAppName(&amp;quot;SingleValueRDD&amp;quot;)
    val sc = new SparkContext(conf)
    val baseRdd: RDD[(String, Int)] = sc.makeRDD(List((&amp;quot;a&amp;quot;, 1), (&amp;quot;a&amp;quot;, 2), (&amp;quot;a&amp;quot;, 3), (&amp;quot;b&amp;quot;, 4)))
    val sameRdd: RDD[(String, Int)] = sc.makeRDD(List((&amp;quot;a&amp;quot;, 1), (&amp;quot;a&amp;quot;, 2), (&amp;quot;b&amp;quot;, 3), (&amp;quot;b&amp;quot;, 4), (&amp;quot;b&amp;quot;, 5), (&amp;quot;a&amp;quot;, 6)), 2)


    /**
     * partitionBy: 根据指定的分区规则对数据进行重新分区
     *    RDD[(Int, Int)] =&amp;gt; PairRDDFunctions 隐式转换
     *    RDD有一个伴生对象，伴生对象中存在：
     *        implicit def rddToPairRDDFunctions[K, V](rdd: RDD[(K, V)])  (implicit kt: ClassTag[K], vt: ClassTag[V], ord: Ordering[K] = null): PairRDDFunctions[K, V] = &amp;#123; new PairRDDFunctions(rdd) &amp;#125;
     *    new HashPartitioner 实质是 val rawMod = x % mod 取摸运算
     *    原本分区： 分区0 (1,1) (2,1)  分区1 (3,1) (4,1)
     *    partitionBy后分区: 分区0 (2,1) (4,1)  分区1 (1,1) (3,1)
     *
     */
//    sc.makeRDD(List(1, 2, 3, 4),2).map((_, 1)).partitionBy(new HashPartitioner(2)).saveAsTextFile(&amp;quot;output&amp;quot;)


    /**
     * reduceByKey: 将数据按照相同的 Key 对 Value 进行两两聚合  若key只有一个就直接返回
     */
    val reduceByKeyRdd: RDD[(String, Int)] = baseRdd.reduceByKey(_ + _)
//    reduceByKeyRdd.collect.foreach(println)  //(a,6) (b,4)

    /**
     * groupByKey: 将数据源中相同key的数据分在一个组中，形成对偶元组
     *        存在shuffle操作，会将数据进行打乱重组  [@href=&amp;quot;https://raw.githubusercontent.com/kanseavegs/picgo_images/master/data/20211013092106.png&amp;quot;]
     *        而spark中shuffle操作必须要落盘操作，不能在内存中数据等待，会导致内存溢出，所以shuffle性能偏低
     *
     *        - groupByKey vs groupBy
     *          val groupByRdd: RDD[(String, Iterable[(String, Int)])] = baseRdd.groupBy(_._1)
     *          groupByRdd.collect.foreach(println) //(a,CompactBuffer((a,1), (a,2), (a,3)))    (b,CompactBuffer((b,4)))
     *
     *        - groupByKey vs reduceByKey
     *          从 shuffle 的角度：都存在shuffle操作，落盘效率均影响性能，但是reduceByKey提前在分区内做了预聚合(提前对分区内的数据进行预聚合，使得罗盘的数据量小了)  https://raw.githubusercontent.com/kanseavegs/picgo_images/master/data/20211013092557.png
     *          从功能的角度：reduceByKey 其实包含分组和聚合的功能。GroupByKey 只能分组
     */
    val groupByKeyRdd: RDD[(String, Iterable[Int])] = baseRdd.groupByKey()
//    groupByKeyRdd.collect.foreach(println)  //(a,CompactBuffer(1, 2, 3))  (b,CompactBuffer(4))

    /**
     * aggregateByKey: 由于reduceByKey分区内combine和分区间shuffle计算【规则相同】，对于某些业务场景可能不适用(比如：分区内求最大值，分区间求和)
     *                 因此aggregateByKey可以做到将数据根据【不同的规则】进行分区内计算和分区间计算
     *                 aggregateByKey存在函数柯里化，需要传递两个参数列表
     *                 - 第一个参数列表需要传递一个初始值，主要用于当碰见第一个key时，和value进行分区内计算
     *                    一定要设置一个初始值和第一个值进行比较
     *                    [@href=&amp;quot;https://raw.githubusercontent.com/kanseavegs/picgo_images/master/data/20211013094155.png&amp;quot;]
     *                    [@href=&amp;quot;https://raw.githubusercontent.com/kanseavegs/picgo_images/master/data/20211013094304.png&amp;quot;]
     *                 - 第二个参数列表需要传递2个参数
     *                    - 第1个参数表示分区内的计算规则
     *                    - 第2个参数表示分区间的计算规则
     *
     */
    val aggregateByKeyRdd: RDD[(String, Int)] = baseRdd.aggregateByKey(0)(
      (x, y) =&amp;gt; math.max(x, y),
      (x, y) =&amp;gt; x + y
    )
//    baseRdd.aggregateByKey(0)(math.max(_,_),_+_)
//    aggregateByKeyRdd.collect.foreach(println) //(a,6), (b,4)


    /**
     * foldByKey: 进行聚合计算时，分区内和分区间计算规则相同时，提供的一种简化方法
     *            val aggregateByKeySameRuleRdd: RDD[(String, Int)] = baseRdd.aggregateByKey(0)(
     *             (x, y) =&amp;gt; x + y,
     *             (x, y) =&amp;gt; x + y
     *             )
     */
    val foldByKeyRdd: RDD[(String, Int)] = baseRdd.foldByKey(0)(_ + _)
//    foldByKeyRdd.collect.foreach(println)


    //aggregateByKey求平均值
//    val avageRdd: RDD[(String, (Int, Int))] = sameRdd.aggregateByKey((0, 0))( // (0,0) 表示使用元组做初始值，第1个0表示数值初始值，第2个0表示数量初始值
//      //分区内
//      (tp, v) =&amp;gt; &amp;#123;
//        (tp._1 + v, tp._2 + 1)
//      &amp;#125;, // (相同key的value相加，相同key的数量加一)
//      //分区间
//      (tp1, tp2) =&amp;gt; &amp;#123;
//        (tp1._1 + tp2._1, tp1._2 + tp2._2)
//      &amp;#125;) //（相同key的分区和相加，相同key的数量叠加）
//      avageRdd.mapValues( tp =&amp;gt; tp._1 /tp._2 ).collect.foreach(println)//只对map中相同key的value进行操作  (a,3), (b,4)

    /**
     * combineByKey: 类似于aggregateByKey，转换相同key的第一个数据结构，不用设置初始值
     */
//    sameRdd.combineByKey(
//      //将相同key的第一个数据结构进行结构的转换，实现操作
//      v =&amp;gt; (v,1),
//      //分区内
//      (tp:(Int,Int), v) =&amp;gt; &amp;#123;
//        (tp._1 + v, tp._2 + 1)
//      &amp;#125;,
//      //分区间
//      (tp1:(Int,Int), tp2:(Int,Int)) =&amp;gt; &amp;#123;
//        (tp1._1 + tp2._1, tp1._2 + tp2._2)
//      &amp;#125;).mapValues( tp =&amp;gt; tp._1 /tp._2 ).collect.foreach(println)

      //reduceByKey、foldByKey、aggregateByKey、combineByKey【区别】
      //reduceByKey: 相同 key 的第一个数据不进行任何计算，分区内和分区间计算规则相同
      //FoldByKey: 相同 key 的第一个数据和初始值进行分区内计算，分区内和分区间计算规则相同
      //AggregateByKey：相同 key 的第一个数据和初始值进行分区内计算，分区内和分区间计算规则可以不相同
      //CombineByKey:当计算时，发现数据结构不满足要求时，可以让第一个数据转换结构。分区内和分区间计算规则不相同


    /**
     * join: 相同 key 对应的所有元素连接在一起
     * 如果两个数据源中key没有匹配上，则不会出现在结果中
     * 如果两个数据源中具有相同key，则结果中会产生笛卡尔积
     */
    val rdd1: RDD[(String, Int)] = sc.makeRDD(List((&amp;quot;a&amp;quot;, 1), (&amp;quot;b&amp;quot;, 2), (&amp;quot;c&amp;quot;, 3)))
    val rdd2: RDD[(String, Int)] = sc.makeRDD(List((&amp;quot;a&amp;quot;, 4), (&amp;quot;b&amp;quot;, 5), (&amp;quot;c&amp;quot;, 6)))
    val joinRdd: RDD[(String, (Int, Int))] = rdd1.join(rdd2)
//    joinRdd.collect.foreach(println) // (a,(1,4))  (b,(2,5))    (c,(3,6))
//    rdd1.join(sc.makeRDD(List((&amp;quot;d&amp;quot;, 3), (&amp;quot;a&amp;quot;, 1)))).collect.foreach(println) //(a,(1,1))  如果两个数据源中key没有匹配上，则不会出现在结果中


    /**
     * leftOuterJoin: 类似于 SQL 语句的左外连接
     * (a,(1,Some(4)))
     * (b,(2,Some(5)))
     * (c,(3,None))
     */
//    sc.makeRDD(List((&amp;quot;a&amp;quot;, 1), (&amp;quot;b&amp;quot;, 2), (&amp;quot;c&amp;quot;, 3))).leftOuterJoin(sc.makeRDD(List((&amp;quot;a&amp;quot;, 4), (&amp;quot;b&amp;quot;, 5)))).collect.foreach(println)

    /**
     * rightOuterJoin: 类似于 SQL 语句的右外连接
     * (a,(Some(1),4))
     * (b,(Some(2),5))
     * (c,(None,6))
     */
//    sc.makeRDD(List((&amp;quot;a&amp;quot;, 1), (&amp;quot;b&amp;quot;, 2))).rightOuterJoin(sc.makeRDD(List((&amp;quot;a&amp;quot;, 4), (&amp;quot;b&amp;quot;, 5),(&amp;quot;c&amp;quot;, 6)))).collect.foreach(println)

    /**
     * cogroup: connect + group : 先分组，再连接
     * (a,(CompactBuffer(1),CompactBuffer(4)))
     * (b,(CompactBuffer(2),CompactBuffer(5)))
     * (c,(CompactBuffer(),CompactBuffer(6, 7)))
     */
//    sc.makeRDD(List((&amp;quot;a&amp;quot;, 1), (&amp;quot;b&amp;quot;, 2))).cogroup(sc.makeRDD(List((&amp;quot;a&amp;quot;, 4), (&amp;quot;b&amp;quot;, 5), (&amp;quot;c&amp;quot;, 6), (&amp;quot;c&amp;quot;, 7)))).collect.foreach(println)

  &amp;#125;
&amp;#125;

&lt;/code&gt;&lt;/pre&gt;
</content>
        <category term="Spark" />
        <updated>2021-10-22T06:07:10.000Z</updated>
    </entry>
</feed>
