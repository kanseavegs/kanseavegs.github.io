{
    "version": "https://jsonfeed.org/version/1",
    "title": "Yuan's Library • All posts by \"spark\" tag",
    "description": "",
    "home_page_url": "http://spark.kanseaveg.xyz",
    "items": [
        {
            "id": "http://spark.kanseaveg.xyz/2021/10/22/SparkCore-%E5%B9%BF%E5%91%8A%E4%BD%BF%E7%94%A8%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90/",
            "url": "http://spark.kanseaveg.xyz/2021/10/22/SparkCore-%E5%B9%BF%E5%91%8A%E4%BD%BF%E7%94%A8%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90/",
            "title": "SparkCore-广告使用量统计分析",
            "date_published": "2021-10-22T06:24:49.000Z",
            "content_html": "<h3 id=\"sparkcore-广告使用量统计分析\"><a class=\"markdownIt-Anchor\" href=\"#sparkcore-广告使用量统计分析\">#</a> SparkCore - 广告使用量统计分析</h3>\n<h5 id=\"adsrdddemoscala\"><a class=\"markdownIt-Anchor\" href=\"#adsrdddemoscala\">#</a> AdsRDDdemo.scala</h5>\n<pre><code>package com.kanseaveg.rdd\n\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.&#123;SparkConf, SparkContext&#125;\n\n/**\n * 案例实操  https://raw.githubusercontent.com/kanseavegs/picgo_images/master/data/%E5%AE%9E%E9%99%85%E6%A1%88%E4%BE%8B-1.png\n * agent.log：时间戳，省份，城市，用户，广告，中间字段使用空格分隔。\n * 需求描述: 统计出每一个省份每个广告被点击数量排行的 Top3\n */\nobject AdsRDDdemo &#123;\n  def main(args: Array[String]): Unit = &#123;\n\n    val conf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;AdsRDDdemo&quot;)\n    val sc = new SparkContext(conf)\n    //1.从agent.log中获取原始数据：时间戳，省份，城市，用户，广告\n    val rawData: RDD[String] = sc.textFile(&quot;datas/agent.log&quot;)\n    //2.将原始数据进行初步清晰，方便统计 ： 时间戳，省份，城市，用户，广告 to ((省份，广告),1)\n    val dataRdd: RDD[((String, String), Int)] = rawData.map(line =&gt; &#123;\n      val eachLine: Array[String] = line.split(&quot; &quot;)\n      ((eachLine(1), eachLine(4)), 1)\n    &#125;)\n    //3.将转换结构后的数据进行分组聚合 ： ((省份，广告),1) to ((省份，广告),sum)\n    val reduceRdd: RDD[((String, String), Int)] = dataRdd.reduceByKey(_ + _)\n    //4.将聚合的结果进行结构的转换 ((省份，广告),sum) =&gt; (省份，(广告,sum)) ,三种方法均可\n\n    val mapRdd: RDD[(String, (String, Int))] = reduceRdd.map( data =&gt; &#123; (data._1._1,(data._1._2,data._2)) &#125;)\n//    val mapRdd: RDD[(String, (String, Int))] = reduceRdd.map(\n//      data =&gt; &#123;\n//        val tp: (String, String) = data._1\n//        (tp._1, (tp._2, data._2))\n//      &#125;)\n//    val mapRdd: RDD[(String, (String, Int))] = reduceRdd.map &#123; case ((province, ads), sum) =&gt; &#123;\n//      (province, (ads, sum))\n//    &#125;&#125;\n\n\n\n    //5.将转换结构后的数据根据省份进行分组 (省份，[(广告A,sumA),广告B,sumB)])\n    val groupRdd: RDD[(String, Iterable[(String, Int)])] = mapRdd.groupByKey()\n    //6.将扥组后的数据组内降序排序，取前三名打印到控制台\n    val resultRdd: RDD[(String, List[(String, Int)])] = groupRdd.mapValues(iter =&gt; &#123;\n      iter.toList.sortBy(-_._2).take(3)\n    &#125;) //key不变,对value进行排序 ,-号表示倒序\n    resultRdd.collect().foreach(println)\n\n\n    /**\n     * 一行写法\n     * sc.textFile(&quot;datas/agent.log&quot;).map( l =&gt; ((l.split(&quot; &quot;)(1),l.split(&quot; &quot;)(4)),1))\n     * .reduceByKey(_+_).map(d=&gt;(d._1._1,(d._1._2,d._2))).groupByKey()\n     * .mapValues(_.toList.sortBy(-_._2).take(3)).collect().foreach(println)\n     */\n\n    /**\n     * (省份, 每省前三名(广告x,点击数量))\n     * (4,List((12,25), (2,22), (16,22)))\n     * (8,List((2,27), (20,23), (11,22)))\n     * (6,List((16,23), (24,21), (22,20)))\n     * (0,List((2,29), (24,25), (26,24)))\n     * (2,List((6,24), (21,23), (29,20)))\n     * (7,List((16,26), (26,25), (1,23)))\n     * (5,List((14,26), (21,21), (12,21)))\n     * (9,List((1,31), (28,21), (0,20)))\n     * (3,List((14,28), (28,27), (22,25)))\n     * (1,List((3,25), (6,23), (5,22)))\n     */\n\n  &#125;\n\n&#125;\n\n</code></pre>\n<h5 id=\"hotcategorytop10analysisscala\"><a class=\"markdownIt-Anchor\" href=\"#hotcategorytop10analysisscala\">#</a> HotCategoryTop10Analysis.scala</h5>\n<pre><code>package com.kanseaveg.rdd\n\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.&#123;SparkConf, SparkContext&#125;\n\n/**\n * top10热门品类分析：\n * 分别统计每个品类点击的次数，下单的次数和支付的次数，并按照（点击总数，下单总数，支付总数）排序\n * （品类，点击总数）（品类，下单总数）（品类，支付总数）\n */\nobject HotCategoryTop10Analysis &#123;\n  def main(args: Array[String]): Unit = &#123;\n    val conf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;aaaa&quot;)\n    val sc = new SparkContext(conf)\n    //1.读取原始日志数据\n    val dataRdd: RDD[String] = sc.textFile(&quot;datas/user_visit_action.csv&quot;)\n    dataRdd.cache()\n    /**\n     * 数据中包含用户的 4 种行为：搜索，点击，下单，支付：\n     *    1.每一行数据表示用户的一次行为，这个行为只能是 4 种行为的一种\n     *    2.如果搜索关键字为 null,表示数据不是搜索数据\n     *    3.如果点击的品类 ID 和产品 ID 为-1，表示数据不是点击数据\n     *    4.对于下单行为，一次可以下单多个商品，品类ID和产品ID可以是多个，id之间采用逗号分隔，如果本次不是下单行为则数据采用 null表示\n     *    5.支付行为和下单行为类似\n     *\n     * 预处理，数据采用下划线分隔数据\n     * 行为日期\t用户 ID\tSession的ID\t页面id\t动作的时间点\n     * 【搜索关键词\t品类ID 商品 ID\t一次订单中所有品类ID集合\t一次订单中所有商品的ID集合\t一次支付中所有品类的ID集合\t一次支付中所有商品的ID集合】 城市id\n     *    5          6      7       8                   9                       10                      11\n     */\n\n    //2.统计品类的点击数量： （品类id-点击数量）\n    val clickRdd: RDD[(String, Int)] = dataRdd.filter(data =&gt; &#123;\n      val line: Array[String] = data.split(&quot;,&quot;)\n      line(6) != &quot;-1&quot; &amp;&amp; line(7) != &quot;-1&quot; //点击的品类 ID 和产品 ID 为-1，表示数据不是点击数据，过滤不是点击的数据\n    &#125;).map(data =&gt; (data.split(&quot;,&quot;)(6), 1)).reduceByKey(_ + _) //（品类id-点击数量）\n\n\n    //3.统计品类的下单数量： （品类id-下单数量）\n    val orderRdd: RDD[(String, Int)] = dataRdd.filter(data =&gt; &#123;\n      val line: Array[String] = data.split(&quot;,&quot;)\n      line(8) != &quot;null&quot; &amp;&amp; line(9) != &quot;null&quot; //品类ID和产品ID可以是多个，如果本次不是下单行为则数据采用 null表示\n    &#125;).flatMap(data =&gt; (data.split(&quot;,&quot;)(8).split(&quot;-&quot;).map(category_id =&gt; (category_id, 1)))) //扁平化，将品类id集合打散，然后写成（category_id，1）的形式\n      .reduceByKey(_ + _)\n\n    //4.统计品类的支付数量： （品类id-支付数量）\n    //支付行为和下单行为类似\n    val payRdd: RDD[(String, Int)] = dataRdd.filter(data =&gt; &#123;\n      val line: Array[String] = data.split(&quot;,&quot;)\n      line(10) != &quot;null&quot; &amp;&amp; line(11) != &quot;null&quot; //品类ID和产品ID可以是多个，如果本次不是支付行为则数据采用 null表示\n    &#125;).flatMap(data =&gt; (data.split(&quot;,&quot;)(10).split(&quot;-&quot;).map(category_id =&gt; (category_id, 1)))) //扁平化，将品类id集合打散，然后写成（category_id，1）的形式\n      .reduceByKey(_ + _)\n\n\n    //已获得clickRdd：（品类id-点击数量）orderRdd：（品类id-下单数量）payRdd：（品类id-支付数量），需要（品类id，（点击数量，下单数量，支付数量））形式\n    //5.将品类进行排序 取出前10名打印  排序规则：点击-下单-支付  使用元组排序即可（品类id，（点击数量，下单数量，支付数量））\n//    val coGroupRdd: RDD[(String, (Iterable[Int], Iterable[Int], Iterable[Int]))] = clickRdd.cogroup(orderRdd, payRdd) //cogroup = connect + group 但是cogroup存在shuffle 性能可能比较低\n//    val analysisRdd: RDD[(String, (Int, Int, Int))] = coGroupRdd.mapValues(tp =&gt; &#123;(tp._1.sum, tp._2.sum, tp._3.sum)&#125;) //（品类id，（点击数量，下单数量，支付数量））\n//    val resultRdd: Array[(String, (Int, Int, Int))] = analysisRdd.sortBy(_._2, false).take(10) //按照（点击数量，下单数量，支付数量）排序 取前10\n\n    /**\n     * cogroup改进\n     * （品类id-点击数量）=&gt;（品类id，（点击数量，0，0））\n     * （品类id-下单数量）=&gt; （品类id，（点击数量，下单数量，0））\n     * （品类id-支付数量）=&gt;（品类id，（点击数量，下单数量，支付数量））\n     */\n    val rdd1: RDD[(String, (Int, Int, Int))] = clickRdd.map(tp =&gt; (tp._1, (tp._2, 0, 0)))\n    val rdd2: RDD[(String, (Int, Int, Int))] = orderRdd.map(tp =&gt; (tp._1, (0, tp._2, 0)))\n    val rdd3: RDD[(String, (Int, Int, Int))] = payRdd.map(tp =&gt; (tp._1, (0, 0, tp._2)))\n    val totalRdd: RDD[(String, (Int, Int, Int))] = rdd1.union(rdd2).union(rdd3)\n    val coGroupReplace: RDD[(String, (Int, Int, Int))] = totalRdd.reduceByKey((a, b) =&gt; (a._1 + b._1, a._2 + b._2, a._3 + b._3))\n    val resultRdd: Array[(String, (Int, Int, Int))] = coGroupReplace.sortBy(_._2, false).take(10) //按照（点击数量，下单数量，支付数量）排序 取前10\n\n\n\n    //6.采集结果并打印\n    resultRdd.foreach(println)\n\n    /**\n     * (20,(3104,0,0))\n     * (15,(3098,0,0))\n     * (17,(3080,0,0))\n     * (14,(3079,0,0))\n     * (10,(3060,0,0))\n     * (13,(3056,0,0))\n     * (2,(3054,6044,4046))\n     * (5,(3032,0,0))\n     * (3,(3024,6044,4046))\n     * (7,(3020,0,0))\n     *\n     */\n\n    sc.stop()\n  &#125;\n\n&#125;\n\n</code></pre>\n<h5 id=\"hotcategorytop10analysis_advancedscala\"><a class=\"markdownIt-Anchor\" href=\"#hotcategorytop10analysis_advancedscala\">#</a> HotCategoryTop10Analysis_Advanced.scala</h5>\n<pre><code>package com.kanseaveg.rdd\n\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.util.AccumulatorV2\nimport org.apache.spark.&#123;SparkConf, SparkContext&#125;\n\nimport scala.collection.mutable\n\n/**\n * 数据中包含用户的 4 种行为：搜索，点击，下单，支付：\n *    1.每一行数据表示用户的一次行为，这个行为只能是 4 种行为的一种\n *    2.如果搜索关键字为 null,表示数据不是搜索数据\n *    3.如果点击的品类 ID 和产品 ID 为-1，表示数据不是点击数据\n *    4.对于下单行为，一次可以下单多个商品，品类ID和产品ID可以是多个，id之间采用逗号分隔，如果本次不是下单行为则数据采用 null表示\n *    5.支付行为和下单行为类似\n *\n * 预处理，数据采用下划线分隔数据\n * 行为日期\t用户 ID\tSession的ID\t页面id\t动作的时间点\n * 【搜索关键词\t品类ID 商品 ID\t一次订单中所有品类ID集合\t一次订单中所有商品的ID集合\t一次支付中所有品类的ID集合\t一次支付中所有商品的ID集合】 城市id\n *    5          6      7       8                   9                       10                      11\n */\n\n/**\n * 2.将数据转换结构\n *    点击的场合：（品类id，（1，0，0））\n *    下单的场合：（品类id，（0，1，0））\n *    支付的场合：（品类id，（0，0，1））\n */\n\n/**\n * top10热门品类分析：\n * 分别统计每个品类点击的次数，下单的次数和支付的次数，并按照（点击总数，下单总数，支付总数）排序\n * （品类，点击总数）（品类，下单总数）（品类，支付总数）\n *\n *\n * 优化方法一：由于reduceBykey存在大量shuffle操作，于是优化预处理，提前将数据格式处理好，但是任然存在一个reducebykey shuffle操作\n * 优化方法二：去除reduceBykey，使用累加器\n */\nobject HotCategoryTop10Analysis_Advanced &#123;\n  def main(args: Array[String]): Unit = &#123;\n    val conf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;aaaa&quot;)\n    val sc = new SparkContext(conf)\n    //1.读取原始日志数据\n    val dataRdd: RDD[String] = sc.textFile(&quot;datas/user_visit_action.csv&quot;)\n\n\n    /**\n     * 优化方法一：由于reduceBykey存在大量shuffle操作，于是优化预处理，提前将数据格式处理好，但是任然存在一个reducebykey shuffle操作\n     */\n//    val flatRdd: RDD[(String, (Int, Int, Int))] = dataRdd.flatMap(data =&gt; &#123;\n//      val cols = data.split(&quot;,&quot;) //按照列拍扁\n//      if (cols(6) != &quot;-1&quot; &amp;&amp; cols(7) != &quot;-1&quot;) &#123;\n//        List((cols(6), (1, 0, 0))) //统计品类的点击数量,点击的品类ID和产品ID不为-1，表示点击数据\n//      &#125; else if (cols(8) != &quot;null&quot; &amp;&amp; cols(9) != &quot;null&quot;) &#123;\n//        cols(8).split(&quot;-&quot;).map(category_id =&gt; (category_id, (0, 1, 0))) //品类ID &quot;1-2-3&quot;\n//      &#125; else if (cols(10) != &quot;null&quot; &amp;&amp; cols(11) != &quot;null&quot;) &#123;\n//        cols(10).split(&quot;-&quot;).map(category_id =&gt; (category_id, (0, 0, 1))) //品类ID &quot;1-2-3&quot;\n//      &#125; else &#123;\n//        Nil\n//      &#125;\n//    &#125;)\n//    val resultRdd: RDD[(String, (Int, Int, Int))] = flatRdd.reduceByKey((a, b) =&gt; (a._1 + b._1, a._2 + b._2, a._3 + b._3)) //    ( 品类ID，( 点击数量, 下单数量, 支付数量 ) )\n\n    /**\n     * 优化方法二：去除reduceBykey，使用自定义累加器\n     */\n    val acc = new HotCategoryAccumulator\n    sc.register(acc,&quot;HotCategoryAcc&quot;)\n    dataRdd.foreach(data =&gt; &#123;\n      val cols = data.split(&quot;,&quot;) //按照列拍扁\n\n      if (cols(6) != &quot;-1&quot; &amp;&amp; cols(7) != &quot;-1&quot;) &#123;\n        acc.add((cols(6),&quot;click&quot;))\n      &#125; else if (cols(8) != &quot;null&quot; &amp;&amp; cols(9) != &quot;null&quot;) &#123;\n        cols(8).split(&quot;-&quot;).foreach(cid =&gt; &#123;acc.add((cid,&quot;order&quot;))&#125;)\n      &#125; else if (cols(10) != &quot;null&quot; &amp;&amp; cols(11) != &quot;null&quot;) &#123;\n        cols(10).split(&quot;-&quot;).foreach(cid =&gt; &#123;acc.add((cid,&quot;pay&quot;))&#125;)\n      &#125; else &#123;\n        Nil\n      &#125;\n    &#125;)\n\n    val allHotCategories: mutable.Iterable[HotCategory] = acc.value.map(_._2)\n    val resultRdd: List[HotCategory] = allHotCategories.toList.sortWith((left, right) =&gt; &#123;\n      if (left.clickCnt &gt; right.clickCnt) true\n      else if (left.clickCnt == right.clickCnt) &#123;\n        if (left.orderCnt &gt; right.orderCnt) true\n        else if (left.orderCnt == right.orderCnt) &#123;\n          if (left.payCnt &gt; right.payCnt) true\n          else false\n        &#125; else false\n      &#125; else false\n    &#125;).take(10)\n\n\n\n\n    //采集结果并打印\n    resultRdd.foreach(println)\n\n    /**\n     * (20,(3104,0,0))\n     * (15,(3098,0,0))\n     * (17,(3080,0,0))\n     * (14,(3079,0,0))\n     * (10,(3060,0,0))\n     * (13,(3056,0,0))\n     * (2,(3054,6044,4046))\n     * (5,(3032,0,0))\n     * (3,(3024,6044,4046))\n     * (7,(3020,0,0))\n     *\n     */\n\n    sc.stop()\n  &#125;\n  case class HotCategory(cid: String, var clickCnt: Int, var orderCnt: Int, var payCnt: Int)\n\n  /**\n   * IN: (品类id，行为类型)\n   * OUT: Map[String, HotCategory]\n   */\n  class HotCategoryAccumulator extends AccumulatorV2[(String,String),mutable.Map[String, HotCategory]] &#123;\n\n    private val hcMap = mutable.Map[String,HotCategory]()\n\n    override def isZero: Boolean = hcMap.isEmpty\n\n    override def copy(): AccumulatorV2[(String, String), mutable.Map[String, HotCategory]] = new HotCategoryAccumulator()\n\n    override def reset(): Unit = hcMap.clear()\n\n    override def add(v: (String, String)): Unit = &#123;\n      val cid: String = v._1 //获取品类id\n      val actionType: String = v._2 //获取需要累加的类型\n      val category: HotCategory = hcMap.getOrElse(cid, HotCategory(cid, 0, 0, 0))\n\n      if (actionType == &quot;click&quot;) &#123;\n        category.clickCnt += 1\n      &#125; else if (actionType == &quot;order&quot;) &#123;\n        category.orderCnt += 1\n      &#125; else if (actionType == &quot;pay&quot;) &#123;\n        category.payCnt += 1\n      &#125;\n      hcMap.update(cid,category)\n    &#125;\n\n    override def merge(other: AccumulatorV2[(String, String), mutable.Map[String, HotCategory]]): Unit = &#123;\n      val map1 = this.hcMap\n      val map2 = other.value\n\n      //对于map2中每一个元素\n      map2.foreach( data =&gt; &#123;\n        val cid: String = data._1\n        val hcc: HotCategory = data._2 //other's Map's HotCategory\n        val category: HotCategory = map1.getOrElse(cid, HotCategory(cid, 0, 0, 0))\n        category.clickCnt += hcc.clickCnt\n        category.orderCnt += hcc.orderCnt\n        category.payCnt += hcc.payCnt\n        map1.update(cid, category)\n      &#125;)\n    &#125;\n\n    override def value: mutable.Map[String, HotCategory] = hcMap\n  &#125;\n\n\n&#125;\n\n</code></pre>\n<h5 id=\"hotcategorytop10sessionanalysisscala\"><a class=\"markdownIt-Anchor\" href=\"#hotcategorytop10sessionanalysisscala\">#</a> HotCategoryTop10SessionAnalysis.scala</h5>\n<pre><code>import org.apache.spark.rdd.RDD\nimport org.apache.spark.&#123;SparkConf, SparkContext&#125;\n\n/**\n * 数据中包含用户的 4 种行为：搜索，点击，下单，支付：\n *    1.每一行数据表示用户的一次行为，这个行为只能是 4 种行为的一种\n *    2.如果搜索关键字为 null,表示数据不是搜索数据\n *    3.如果点击的品类 ID 和产品 ID 为-1，表示数据不是点击数据\n *    4.对于下单行为，一次可以下单多个商品，品类ID和产品ID可以是多个，id之间采用逗号分隔，如果本次不是下单行为则数据采用 null表示\n *    5.支付行为和下单行为类似\n *\n * 预处理，数据采用下划线分隔数据\n * 行为日期\t用户 ID\tSession的ID\t页面id\t动作的时间点\n * 【搜索关键词\t品类ID 商品 ID\t一次订单中所有品类ID集合\t一次订单中所有商品的ID集合\t一次支付中所有品类的ID集合\t一次支付中所有商品的ID集合】 城市id\n *    5          6      7       8                   9                       10                      11\n */\n\n/**\n * top10热门品类分析：\n * Top10 热门品类中每个品类的 Top10 活跃 Session 统计\n */\nobject HotCategoryTop10SessionAnalysis &#123;\n  def main(args: Array[String]): Unit = &#123;\n    val conf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;aaaa&quot;)\n    val sc = new SparkContext(conf)\n    val dataRdd: RDD[String] = sc.textFile(&quot;datas/user_visit_action.csv&quot;)\n\n    val top10Ids: Array[String] = top10Category(dataRdd)\n    //过滤Top10热门品类中的点击品类Ids\n    val clickCidsOfTop10: RDD[String] = dataRdd.filter(data =&gt; &#123;\n      val cols: Array[String] = data.split(&quot;,&quot;)\n      cols(6) != &quot;-1&quot; &amp;&amp; cols(7) != &quot;-1&quot; &amp;&amp; top10Ids.contains(cols(6)) //  点击的品类ID和产品 ID不为-1\n    &#125;)\n\n    //根据品类id和sessionid进行点击量统计\n    val reduceRdd: RDD[((String, String), Int)] = clickCidsOfTop10.map(data =&gt; &#123;\n      val cols: Array[String] = data.split(&quot;,&quot;)\n      ((cols(6), cols(2)), 1) // ((品类ID，sessionId),1))\n    &#125;).reduceByKey(_ + _) //// ((品类ID，sessionId),sum))\n\n\n    // 将统计的结果进行结构的转换 转换后进行分组聚合  ((品类ID，sessionId),sum)) to  (品类ID，(sessionId,sum)))\n    val groupRdd: RDD[(String, Iterable[(String, Int)])] = reduceRdd.map(data =&gt; (data._1._1, (data._1._2, data._2))).groupByKey() //  (品类ID，(sessionId,sum))) 按照品类id分组\n\n    //将分组后的数据进行点击量的排序，取前10名\n    val resultRdd: RDD[(String, List[(String, Int)])] = groupRdd.mapValues(iter =&gt; iter.toList.sortBy(_._2)(Ordering.Int.reverse).take(10)) //按照sum排序\n\n    resultRdd.foreach(println)\n\n    /**\n     * (4,List((5310bea7-9d61-4c9b-bfd7-7176c40a7d1f,6), (82956a11-86bc-4ab9-a7c5-2c602d5da84d,5), (83df2649-0b85-41d9-a1db-21392954b83d,5), (32f6cee8-4dc9-49e0-a05d-dbc0c1c9daae,5), (fd9a3aae-f5c2-425f-bb8f-8ed19db009fa,4), (9e3b5ab9-ba3e-4334-92df-6d851db2b75f,4), (0fb43e3d-78dd-474f-8892-08fb2c0a5132,4), (b2741f4f-a071-464c-bbc0-baf863fdc767,4), (23087483-70c1-4e5f-9d53-5f7020032929,4), (9e724bb6-8903-45d8-a18e-69e8386e6bad,4)))\n     * (8,List((040c420b-6709-4aa3-818e-4bfdfac59640,5), (9e5db7d9-4616-46bd-9cef-de671d113925,5), (9e35a818-784b-4723-8d1d-a19acda88f29,5), (06ad898c-0390-452a-88e7-2ca1c1d3355c,5), (0594ee8f-499d-49c6-8328-aa756467a2ad,5), (b3c63508-d10d-427e-a797-d44b172a40b1,5), (c7f1bea8-94dc-444c-b1e6-4b767c7c4f13,4), (dd89c2b4-d6f1-4112-b705-6ce263abdfd4,4), (57500425-a1d6-4dad-8ec4-224e443e567f,4), (738447c6-e65b-4fd9-8158-d244245b6633,4)))\n     * (20,List((710373f5-3a2e-4fec-9ddb-623d779273e6,6), (f632bbe4-e5ba-4dcf-91ef-89f9688376a9,6), (7d0014a4-c501-456f-9d30-fe6af79d933c,6), (3208994d-5867-4c8f-841f-42700aeabfb3,6), (c34d6503-b62b-4bab-9b85-7b0aefd8a5e0,5), (c9f87d6f-6a62-4e20-a2bb-aa0b690c134c,5), (78bc6698-ac26-4f1b-aaa0-05232f3322e5,5), (7ea420e7-eaa4-42dd-b517-470656fb87b6,5), (1c5e83e4-370c-4163-a387-6d61a0233e24,5), (8b9a0f5a-bff9-4f37-b3bb-6aa3c24d24f7,5)))\n     * (19,List((8b9a0f5a-bff9-4f37-b3bb-6aa3c24d24f7,7), (72b227ad-aaad-4c6c-965f-d2ed3428aa20,6), (592c3702-9886-4725-9869-a43cf972a7dc,5), (e068b484-7f57-4e7e-9db8-d6842ce43858,5), (6b53902b-93e2-4e08-be23-da9e12699dc5,5), (0700b3ec-90b1-42be-bc08-ae9a0081cfe9,4), (7d0014a4-c501-456f-9d30-fe6af79d933c,4), (dfa98b65-2ebc-4215-b080-0aff2f57d89a,4), (a5975ee5-91b1-4e22-b6f0-d4a12f8fd4ed,4), (577e5749-3594-448c-9771-e5f398a9cbb3,4)))\n     * (15,List((8b9a0f5a-bff9-4f37-b3bb-6aa3c24d24f7,8), (e01a941e-08e7-4fd6-aa77-eb10b5c2644b,8), (4c840047-6302-4e7d-8505-f5a0ac11da2f,5), (dd89c2b4-d6f1-4112-b705-6ce263abdfd4,5), (1a163c8d-3d58-4a76-8b28-d732bb51aec8,5), (57500425-a1d6-4dad-8ec4-224e443e567f,5), (0f0c33f2-a9c9-4264-befe-dce0d892526a,5), (f9936a33-3cad-4ac0-8153-77234c6cdb09,5), (d988514f-f1cd-4ddf-84b5-e11fb536e97a,4), (75cf4d14-87f4-4e39-8050-c3d164a002bc,4)))\n     * (6,List((8b9a0f5a-bff9-4f37-b3bb-6aa3c24d24f7,7), (bc9b9ad6-cff4-490e-a405-4c5b3b284fcb,6), (a5400810-3480-4fea-ba5a-3c77170082c3,5), (b7a3d1d4-fa2b-48c8-8bec-e07c59e02c6c,5), (f379a991-0ebc-4817-898d-86affa518835,4), (8b3c224f-66f7-48b6-a9e2-a8861968114c,4), (c1d705bf-bfb2-4a19-94af-c32ea0d5885d,4), (49e2529d-ff6d-46a8-9742-42c01170e394,4), (6dc1bfb3-92b4-44c4-b790-5fbc7ca414ed,4), (7328ce83-2c98-4d39-a1bf-1af472bb8c68,4)))\n     * (2,List((36003baf-6d64-49be-a047-9fc8c0e76ca8,6), (233e790a-1d96-4559-9205-471dc235161d,5), (1ecc15fb-85e4-4c0f-b68f-7351d9f77895,5), (e222b39c-8b13-4865-a8ba-892bd8919b5b,5), (72b227ad-aaad-4c6c-965f-d2ed3428aa20,5), (e5a2e03a-e650-446e-bff2-4845dc726278,5), (0d10c830-c48c-418c-b51b-66d1130b4b57,5), (fd1471b0-1b95-465f-95c7-256ec3e65e26,5), (fd6b98aa-6662-4d30-b63d-683264bf4a3d,5), (285278ad-affb-4b17-a442-3b83dae12576,5)))\n     * (17,List((6b53902b-93e2-4e08-be23-da9e12699dc5,7), (39b5df21-1c7f-4a8a-9766-84ee5f6954e0,6), (f632bbe4-e5ba-4dcf-91ef-89f9688376a9,6), (b21551c7-40e1-405c-81e2-d7caf53e6c07,5), (6501d135-cf17-4020-9852-51b6c3d5f1d7,5), (36003baf-6d64-49be-a047-9fc8c0e76ca8,5), (595f8ca5-7d03-4a32-956e-894821cd17f9,5), (8828d848-74b6-4bea-9a12-4c4d9f6e20c8,5), (5a841cd1-99ed-47fa-af9b-72183fd37b67,5), (7328ce83-2c98-4d39-a1bf-1af472bb8c68,5)))\n     * (13,List((d4125ad2-454d-40b6-b0d6-769ff58a26b0,6), (23087483-70c1-4e5f-9d53-5f7020032929,6), (0031c47d-1bcf-43d9-ad6a-252339b47082,5), (a550b6e4-6e9f-45f8-86ec-040919295efd,5), (7a20dbce-58a5-464b-86e6-7abef1cedd64,5), (8b9a0f5a-bff9-4f37-b3bb-6aa3c24d24f7,5), (0f0c33f2-a9c9-4264-befe-dce0d892526a,5), (70e2e082-08e6-4e14-bec6-6ac266e3bf90,5), (fe75781f-0aa1-4cfd-a11f-128ee35707db,5), (9584d531-47b6-455c-9843-24e45f47939b,5)))\n     *\n     */\n\n    sc.stop()\n  &#125;\n  def top10Category(dataRdd:RDD[String]) = &#123;\n    val flatRdd: RDD[(String, (Int, Int, Int))] = dataRdd.flatMap(data =&gt; &#123;\n      val cols = data.split(&quot;,&quot;) //按照列拍扁\n      if (cols(6) != &quot;-1&quot; &amp;&amp; cols(7) != &quot;-1&quot;) &#123;\n        List((cols(6), (1, 0, 0))) //统计品类的点击数量,点击的品类ID和产品ID不为-1，表示点击数据\n      &#125; else if (cols(8) != &quot;null&quot; &amp;&amp; cols(9) != &quot;null&quot;) &#123;\n        cols(8).split(&quot;-&quot;).map(category_id =&gt; (category_id, (0, 1, 0))) //品类ID &quot;1-2-3&quot;\n      &#125; else if (cols(10) != &quot;null&quot; &amp;&amp; cols(11) != &quot;null&quot;) &#123;\n        cols(10).split(&quot;-&quot;).map(category_id =&gt; (category_id, (0, 0, 1))) //品类ID &quot;1-2-3&quot;\n      &#125; else &#123;\n        Nil\n      &#125;\n    &#125;)\n    val resRdd: RDD[(String, (Int, Int, Int))] = flatRdd.reduceByKey((a, b) =&gt; (a._1 + b._1, a._2 + b._2, a._3 + b._3)) //    ( 品类ID，( 点击数量, 下单数量, 支付数量 ) )\n    resRdd.take(10).map(_._1)//只需要前10的品类ID\n  &#125;\n\n&#125;\n\n</code></pre>\n",
            "tags": [
                "Spark"
            ]
        },
        {
            "id": "http://spark.kanseaveg.xyz/2021/10/22/SparkCore-ACC%E7%B4%AF%E5%8A%A0%E5%99%A8/",
            "url": "http://spark.kanseaveg.xyz/2021/10/22/SparkCore-ACC%E7%B4%AF%E5%8A%A0%E5%99%A8/",
            "title": "SparkCore-ACC累加器",
            "date_published": "2021-10-22T06:07:10.000Z",
            "content_html": "<h3 id=\"sparkcore-acc累加器\"><a class=\"markdownIt-Anchor\" href=\"#sparkcore-acc累加器\">#</a> SparkCore-ACC 累加器</h3>\n<pre><code>package com.kanseaveg.rdd\n\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.util.&#123;AccumulatorV2, LongAccumulator&#125;\nimport org.apache.spark.&#123;SparkConf, SparkContext&#125;\n\nimport scala.collection.mutable\n\n/**\n * 累加器ACC：\n * [@href=&quot;https://raw.githubusercontent.com/kanseavegs/picgo_images/master/data/20211015140254.png&quot;]\n */\nobject ACC &#123;\n  def main(args: Array[String]): Unit = &#123;\n\n    val conf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;AdsRDDdemo&quot;)\n    val sc = new SparkContext(conf)\n    val rdd: RDD[Int] = sc.makeRDD(List(1, 2, 3, 4), 2)\n    //全部分区数据求和\n//    println(rdd.reduce(_ + _)) //reduce包括分区内的计算和分区间的计算\n//    var sum = 0\n//    rdd.foreach( num =&gt; sum+=num) // [@href=&quot;https://raw.githubusercontent.com/kanseavegs/picgo_images/master/data/20211015141205.png&quot;]\n//    println(sum)  // sum竟然等于0！竟然等于0！竟然等于0！ 因为此时数据分发到各个Executor上运行(1+2=3),(3+4=7)，但是并没有返回汇总,Driver段sum依旧是0\n\n    //获取系统的累加器：Spark默认就提供了简单数据聚合的累加器\n    val sumAcc: LongAccumulator = sc.longAccumulator(&quot;sum&quot;) //分布式共享只写变量\n    rdd.foreach(\n      num =&gt; sumAcc.add(num)\n    )\n//    println(sumAcc.value)\n\n    /**\n     * 自定义累加器: 实现wordcount shuffle前提前将数据和汇总\n     */\n    val words: RDD[String] = sc.makeRDD(List(&quot;hello scala&quot;, &quot;hello spark&quot;))\n    val wcAcc: MyAccumulator = new MyAccumulator() //创建累加器对象\n    sc.register(wcAcc,&quot;wordCountACC&quot;) //向spark上下文进行注册\n    words.foreach(word =&gt; wcAcc.add(word)) //使用累加器对数据进行累加\n    println(wcAcc.value) //Map(hello spark -&gt; 1, hello scala -&gt; 1)\n\n    sc.stop()\n  &#125;\n\n  /**\n   * 自定义累加器 1.继承AccumulatorV2类并传入输入输出类型\n   *            2.实现方法\n   */\n  class MyAccumulator extends AccumulatorV2[String,mutable.Map[String,Long]] &#123;\n\n    private var wcMap = mutable.Map[String,Long]()\n\n    //判断是否为初始状态\n    override def isZero: Boolean = wcMap.isEmpty\n\n    //复制累加器\n    override def copy(): AccumulatorV2[String, mutable.Map[String, Long]] = new MyAccumulator\n\n    override def reset(): Unit = wcMap.clear()\n\n    //获取累加器需要计算的值\n    override def add(word: String): Unit = &#123;\n        wcMap.update(word, wcMap.getOrElse(word,0L) + 1)\n    &#125;\n\n    //Driver合并各个分区多个累加器\n    override def merge(other: AccumulatorV2[String, mutable.Map[String, Long]]): Unit = &#123; //other 表示另一分区的累加器\n      val curAccMap: mutable.Map[String, Long] = this.wcMap\n      val otherAccMap: mutable.Map[String, Long] = other.value\n      otherAccMap.foreach( tp =&gt;  curAccMap.update(tp._1,curAccMap.getOrElse(tp._1,0L) + tp._2)) //把tuple作为整体传入 _1为word,_2为count\n//      otherAccMap.foreach&#123; case (word,count) =&gt; curAccMap.update(word, curAccMap.getOrElse(word,0L)+count)&#125; //老师写法\n    &#125;\n    //累加器返回结果\n    override def value: mutable.Map[String, Long] = &#123;\n      wcMap\n    &#125;\n  &#125;\n&#125;\n</code></pre>\n",
            "tags": [
                "Spark"
            ]
        },
        {
            "id": "http://spark.kanseaveg.xyz/2021/10/22/SparkCore-BroadcastVariable%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/",
            "url": "http://spark.kanseaveg.xyz/2021/10/22/SparkCore-BroadcastVariable%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/",
            "title": "SparkCore-BroadcastVariable广播变量",
            "date_published": "2021-10-22T06:07:10.000Z",
            "content_html": "<h3 id=\"sparkcore-broadcastvariable广播变量\"><a class=\"markdownIt-Anchor\" href=\"#sparkcore-broadcastvariable广播变量\">#</a> SparkCore-BroadcastVariable 广播变量</h3>\n<pre><code>package com.kanseaveg.rdd\n\nimport org.apache.spark.broadcast.Broadcast\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.&#123;SparkConf, SparkContext&#125;\n\n/**\n *                                      【广播变量】\n *    [@href=&quot;https://www.bbsmax.com/A/QW5Yv9rMdm/&quot;] \n *    共享变量出现的原因：通常在向 Spark 传递函数时，比如使用map或reduce传条件或变量时，在driver端定义变量，但是集群中运行的每个任务都会得到这些变量的一份\n *                     新的副本，更新这些副本的值driver端的对应变量并不会随之更新。Spark 的两个共享变量，广播变量与累加器分别为变量提供广播与聚合功能，突破了变量不能共享的限制。\n *    Spark两种共享变量：广播变量（broadcast variable）与累加器（accumulator），广播变量常用来高效分发较大的对象，而累加器用来对信息进行聚合。\n *\n *                                      【广播变量 vs 累加器】\n *    累加器就是只写变量 通常就是做事件统计用的 因为rdd是在不同的excutor去执行的 你在不同excutor中累加的结果 没办法汇总到一起 这个时候就需要累加器来帮忙完成\n *    广播变量是只读变量 正常的话我们在driver定义一个变量 需要序列化 才能在excutor端使用  而且是每个task都需要传输一次 这样如果我们定义的对象很大的话就会产生大量的IO\n *                     如果你把这个大对象定义成广播变量的话 我们只需要每个excutor发送一份就可以 如果task需要时 只需要从excutor拉取就可以了。可以减轻集群driver和executor间的通信压力，节省集群资源。\n *\n * 为什么要使用这个广播变量： 如果我们要在分布式计算里面分发大对象，例如：字典，集合，黑白名单等，这个都会由Driver端进行分发，\n *                        一般来讲，如果这个变量不是广播变量，那么每个task就会分发一份，这在task数目十分多的情况下Driver的带宽会成为系统的瓶颈，\n *                        而且会大量消耗task服务器上的资源，如果将这个变量声明为广播变量，那么知识每个executor拥有一份，这个executor启动的task\n *                        会共享这个变量，节省了通信的成本和服务器的资源。\n */\nobject BroadCastVariable &#123;\n  def main(args: Array[String]): Unit = &#123;\n    val conf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;aaaa&quot;)\n    val sc = new SparkContext(conf)\n\n    /**\n     * sample: 过滤来访ip\n     */\n    val lineRdd = sc.textFile(&quot;datas/ip_list.txt&quot;);\n\n    //使用广播变量时，driver第一次向executor发送task时候，发送blackList，缓存到blockmanager，以后不会再发送\n    //在driver端进行广播blackList\n    val blackList = List[String](&quot;8.8.8.8&quot;,&quot;114.114.114.114&quot;);//定义黑名单ip\n    val broadCast: Broadcast[List[String]] = sc.broadcast(blackList)\n\n    //在executor端用broadCast.value获取blackList的值\n    val filterRdd: RDD[String] = lineRdd.filter(ip =&gt; !broadCast.value.contains(ip))\n    filterRdd.foreach(println)\n\n    sc.stop()\n  &#125;\n&#125;\n</code></pre>\n",
            "tags": [
                "Spark"
            ]
        },
        {
            "id": "http://spark.kanseaveg.xyz/2021/10/22/SparkCore-RDD%E5%8D%95%E5%80%BC%E7%B1%BB%E5%9E%8B/",
            "url": "http://spark.kanseaveg.xyz/2021/10/22/SparkCore-RDD%E5%8D%95%E5%80%BC%E7%B1%BB%E5%9E%8B/",
            "title": "SparkCore-RDD转换算子单值类型",
            "date_published": "2021-10-22T06:07:10.000Z",
            "content_html": "<h3 id=\"sparkcore-rdd转换算子单值类型\"><a class=\"markdownIt-Anchor\" href=\"#sparkcore-rdd转换算子单值类型\">#</a> SparkCore-RDD 转换算子单值类型</h3>\n<pre><code>package com.kanseaveg.rdd\n\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.&#123;SparkConf, SparkContext&#125;\n\n/**\n * 单值转换算子:\n *\n * 分区内数据的执行是有序的\n */\nobject SingleValueRDD &#123;\n  def main(args: Array[String]): Unit = &#123;\n    val conf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;SingleValueRDD&quot;)\n    val sc = new SparkContext(conf)\n    val baseRdd: RDD[Int] = sc.makeRDD(List(1, 2, 3, 4),2)\n    val baseListRdd: RDD[List[Int]] = sc.makeRDD(List(\n      List(1, 2), List(3, 4)\n    ))\n\n    /**\n     * map：逐条转换，串行操作\n     */\n    val mapRdd: RDD[Int] = baseRdd.map(_ * 2)\n//    mapRdd.collect().foreach(println)\n\n    /**\n     * mapPartitions: 设置缓冲，一个分区的数据进行转换，但是需要传入一个迭代器 传回一个迭代器，批处理\n     * 但是会将整个分区加载到内存进行引用，如果处理完的数据没被释放掉，则会存在对象的引用\n     */\n    val mapPartitionsRdd: RDD[Int] = baseRdd.mapPartitions(iter =&gt; List(iter.max).iterator) //取得分区最大值，返回迭代器\n//    mapPartitionsRdd.collect().foreach(println)\n\n    /**\n     * mapPartitionsIndex: 在mappartitions基础上增加了分区索引\n     */\n//    val mapPartitionsIndexRdd: RDD[Int] = baseRdd.mapPartitionsWithIndex( (index,iter) =&gt; &#123; if(index==1) iter else Nil.iterator&#125;); //取出分区1中的数据\n    val mapPartitionsIndexRdd: RDD[(Int,Int)] = baseRdd.mapPartitionsWithIndex( (index,iter) =&gt; iter.map( num =&gt; &#123;(index,num)&#125;)); //取出分区号和分区中的数据\n//    mapPartitionsIndexRdd.foreach(println)\n\n    /**\n     * flatMap: 扁平映射\n     */\n    val flatmapRdd: RDD[Int] = baseListRdd.flatMap(list =&gt; list) //前面的list表示各个List元素，后面那个list表示封装到一个新的list中\n//    flatmapRdd.collect.foreach(println)\n\n    /**\n     * glom:将同一个分区的数据直接转换为相同类型的内存数组进行处理，分区不变\n     *    原来：val baseRdd: RDD[Int] = sc.makeRDD(List(1, 2, 3, 4),2)  数据分布在两个分区中 【拆散】至同一个分区中\n     *    现在：val glomRdd: RDD[Array[Int]] ，同一个分区的数据【合并】至一个数组中\n     *    res:  1,2\n     *          3,4\n     */\n    val glomRdd: RDD[Array[Int]] = baseRdd.glom()\n//    glomRdd.collect.foreach(data=&gt; println(data.mkString(&quot;,&quot;)))\n\n    /**\n     * groupBy:将数据根据指定的规则进行分组, [分区默认不变]，但是数据会被打乱重新组合[shuffle]\n     *        参数：传入一个分组函数，根据返回的分组key进行分组，相同的key值会放入到一个组中\n     *        注意：分组和分区没有必然的关系\n     * (0,CompactBuffer(2, 4))\n     * (1,CompactBuffer(1, 3))\n      */\n    val groupByRdd: RDD[(Int, Iterable[Int])] = baseRdd.groupBy(_ % 2)  //模2相同的放在一个组中\n//    groupByRdd.collect.foreach(println)\n\n    /**\n     * fliter: 过滤\n     *  当数据进行筛选过滤后，分区不变，但是可能会出现数据倾斜问题\n     */\n    val fliterRdd: RDD[Int] = baseRdd.filter(_%2==0)\n//    fliterRdd.collect.foreach(println)\n\n    /**\n     * sample: 样本抽取\n     *        withReplacement:  抽取数据放回/不放回\n     *        fraction: 抽取的几率，范围在[0,1]之间,0：全不取；1：全取,每条数据预期被抽取到的期望概率,基准值\n     *        seed: Long = Utils.random.nextLong): RDD[T] 随机方法的随机数种子\n     */\n    val sampleRdd: RDD[Int] = baseRdd.sample(false, 0.4, 1)\n//    sampleRdd.collect.foreach(println)\n\n    /**\n     * distinct: 去重\n     *  scala中用hashset去重，spark中\n     *  case _ =&gt; map(x =&gt; (x, null)).reduceByKey((x, _) =&gt; x, numPartitions).map(_._1) 相同的key做聚合\n     */\n    val distinctRdd: RDD[Int] = baseRdd.distinct()\n//    distinctRdd.collect.foreach(println)\n\n\n    /**\n     * coalesce: 缩减分区, 收缩合并分区，减少分区的个数，减小任务调度成本， 用于大数据集过滤后，提高小数据集的执行效率\n     *    默认情况下不会将分区的数据进行打乱重新组合[没有shuffle]，会出现数据倾斜问题\n     *    第二个参数：是否进行shuffle操作\n     *    coalesce也可以扩大分区，不过需要传入第二个参数shuffle=true\n     */\n    val coalesceRdd: RDD[Int] = baseRdd.coalesce(1,true) //2个分区缩减为1个分区\n//    coalesceRdd.collect.foreach(println)\n\n    /**\n     * repartition: 扩大分区,内部还是调用 coalesce 操作 参数 shuffle 的默认值为 true\n     */\n    val repartitionRdd: RDD[Int] = baseRdd.repartition(4)\n//    repartitionRdd.collect.foreach(println)\n\n    /**\n     * sortBy: 排序，排序后分区数不变，默认为升序排列, 中间存在 shuffle 的过程【打乱数据】\n     */\n    val sortByRdd: RDD[Int] = baseRdd.sortBy(num =&gt; num)\n//    sortByRdd.collect.foreach(println)\n\n  &#125;\n&#125;\n\n</code></pre>\n",
            "tags": [
                "Spark"
            ]
        },
        {
            "id": "http://spark.kanseaveg.xyz/2021/10/22/SparkCore-RDD%E5%8F%8C%E5%80%BC%E7%B1%BB%E5%9E%8B/",
            "url": "http://spark.kanseaveg.xyz/2021/10/22/SparkCore-RDD%E5%8F%8C%E5%80%BC%E7%B1%BB%E5%9E%8B/",
            "title": "SparkCore-RDD转换算子双值类型",
            "date_published": "2021-10-22T06:07:10.000Z",
            "content_html": "<h3 id=\"sparkcore-rdd转换算子-双值类型\"><a class=\"markdownIt-Anchor\" href=\"#sparkcore-rdd转换算子-双值类型\">#</a> SparkCore-RDD 转换算子 - 双值类型</h3>\n<pre><code>package com.kanseaveg.rdd\n\nimport org.apache.spark.&#123;SparkConf, SparkContext&#125;\nimport org.apache.spark.rdd.RDD\n\n/**\n * 双值转换算子:  两个数据源之间的关联操作\n */\nobject DoubleValueRDD &#123;\n  def main(args: Array[String]): Unit = &#123;\n    val conf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;SingleValueRDD&quot;)\n    val sc = new SparkContext(conf)\n    val rdd1: RDD[Int] = sc.makeRDD(List(1, 2, 3, 4))\n    val rdd2: RDD[Int] = sc.makeRDD(List(3, 4, 5, 6))\n    //交集\n    val intersectionRdd: RDD[Int] = rdd1.intersection(rdd2)\n    println(intersectionRdd.collect().mkString(&quot;,&quot;)) //3,4\n    //并集\n    val unionRdd: RDD[Int] = rdd1.union(rdd2)\n    println(unionRdd.collect().mkString(&quot;,&quot;))  //1,2,3,4,3,4,5,6\n    //差集\n    val subtractRdd: RDD[Int] = rdd1.subtract(rdd2)\n    println(subtractRdd.collect().mkString(&quot;,&quot;))  //1,2\n    //拉链\n    val zipRdd: RDD[(Int, Int)] = rdd1.zip(rdd2)\n    println(zipRdd.collect().mkString(&quot;,&quot;)) //(1,3),(2,4),(3,5),(4,6)\n    \n  &#125;\n&#125;\n</code></pre>\n",
            "tags": [
                "Spark"
            ]
        },
        {
            "id": "http://spark.kanseaveg.xyz/2021/10/22/SparkCore-RDD%E6%8C%81%E4%B9%85%E5%8C%96/",
            "url": "http://spark.kanseaveg.xyz/2021/10/22/SparkCore-RDD%E6%8C%81%E4%B9%85%E5%8C%96/",
            "title": "SparkCore-RDD持久化",
            "date_published": "2021-10-22T06:07:10.000Z",
            "content_html": "<h3 id=\"sparkcore-rdd持久化\"><a class=\"markdownIt-Anchor\" href=\"#sparkcore-rdd持久化\">#</a> SparkCore-RDD 持久化</h3>\n<pre><code>package com.kanseaveg.rdd\n\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.&#123;SparkConf, SparkContext&#125;\n\n/**\n * 持久化RDD : [@href=&quot;https://raw.githubusercontent.com/kanseavegs/picgo_images/master/data/20211015105854.png&quot;]\n */\nobject PersistRDD &#123;\n  def main(args: Array[String]): Unit = &#123;\n\n    val conf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;AdsRDDdemo&quot;)\n    val sc = new SparkContext(conf)\n    val baseRdd: RDD[String] = sc.makeRDD(List(&quot;Hello Scala&quot;, &quot;Hello Spark&quot;))\n\n    /**\n     * 未持久化： 由于RDD不存储数据，如果一个RDD需要重复使用，那么需要从头再次执行来获取数据\n     *            RDD对象是可以重用的，但是数据无法重用\n     */\n    val mapRdd: RDD[(String, Int)] = baseRdd.flatMap(_.split(&quot; &quot;)).map((_, 1))\n//    println(&quot;********复用中间变量********&quot;)\n//    mapRdd.reduceByKey(_+_).collect().foreach(println)\n//    mapRdd.groupByKey().collect().foreach(println)\n\n    /**\n     * cache持久化操作，仍调用persist，默认采用保存至内存中的策略\n     */\n//    println(&quot;********通过 Cache 或者 Persist 方法将前面的计算结果缓存********&quot;)\n//    mapRdd.cache()\n\n    /**\n     * presist持久化操作，可以设定持久化策略，保存在内存中还是磁盘中 StorageLevel\n     */\n//    mapRdd.persist(StorageLevel.DISK_ONLY)\n\n    /**\n     * 持久化操作会在行动算子执行时才会触发\n     */\n//    mapRdd.reduceByKey(_+_).collect().foreach(println)\n//    mapRdd.groupByKey().collect().foreach(println)\n\n    /**\n     * checkpoint检查点持久化操作，需要落盘指定检查点路径\n     *          检查点路径中保存的文件，在作业执行完后 不会被删除\n     */\n//    sc.setCheckpointDir(&quot;cp&quot;)\n//    mapRdd.checkpoint()\n\n    /**\n     * cache vs persist vs checkpoint\n     * cache： 临时存储数据在内存，会在血缘关系中添加新的依赖\n     * persist： ~在磁盘，涉及磁盘IO，性能低但数据安全，作业执行完毕，数据文件会被自动丢弃\n     * checkpoint: ~在磁盘，同样涉及磁盘IO，一般情况下联合cache一齐使用，会独立执行作业\n     */\n\n\n\n\n  &#125;\n&#125;\n</code></pre>\n",
            "tags": [
                "Spark"
            ]
        },
        {
            "id": "http://spark.kanseaveg.xyz/2021/10/22/SparkCore-RDD%E8%A1%80%E7%BC%98%E5%85%B3%E7%B3%BB/",
            "url": "http://spark.kanseaveg.xyz/2021/10/22/SparkCore-RDD%E8%A1%80%E7%BC%98%E5%85%B3%E7%B3%BB/",
            "title": "SparkCore-RDD转换算子-双值类型",
            "date_published": "2021-10-22T06:07:10.000Z",
            "content_html": "<h3 id=\"sparkcore-rdd之间的血缘关系\"><a class=\"markdownIt-Anchor\" href=\"#sparkcore-rdd之间的血缘关系\">#</a> SparkCore-RDD 之间的血缘关系</h3>\n<pre><code>\npackage com.kanseaveg.rdd\n\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.&#123;SparkConf, SparkContext&#125;\n\n/**\n * 血缘关系： [@href=&quot;https://raw.githubusercontent.com/kanseavegs/picgo_images/master/data/20211014154027.png&quot;]\n *\n * ******************************textFile************************\n * (2) datas/word.txt MapPartitionsRDD[1] at textFile at DependencyRDD.scala:15 []              current\n * |  datas/word.txt HadoopRDD[0] at textFile at DependencyRDD.scala:15 []\n * =============================flatmap==========================\n * (2) MapPartitionsRDD[2] at flatMap at DependencyRDD.scala:18 []\n * |  datas/word.txt MapPartitionsRDD[1] at textFile at DependencyRDD.scala:15 []              current\n * |  datas/word.txt HadoopRDD[0] at textFile at DependencyRDD.scala:15 []\n * ******************************map************************\n * (2) MapPartitionsRDD[3] at map at DependencyRDD.scala:21 []\n * |  MapPartitionsRDD[2] at flatMap at DependencyRDD.scala:18 []\n * |  datas/word.txt MapPartitionsRDD[1] at textFile at DependencyRDD.scala:15 []              current\n * |  datas/word.txt HadoopRDD[0] at textFile at DependencyRDD.scala:15 []\n * ==========================reduceByKey==========================\n * (2) ShuffledRDD[4] at reduceByKey at DependencyRDD.scala:24 []              current, but +- represents the shuffle process.\n * +-(2) MapPartitionsRDD[3] at map at DependencyRDD.scala:21 []\n * |  MapPartitionsRDD[2] at flatMap at DependencyRDD.scala:18 []\n * |  datas/word.txt MapPartitionsRDD[1] at textFile at DependencyRDD.scala:15 []\n * |  datas/word.txt HadoopRDD[0] at textFile at DependencyRDD.scala:15 []\n * (Hello,2)\n * (Scala,1)\n * (Spark,1)\n *\n * Process finished with exit code 0\n *\n *\n * oneToOne依赖： 新的RDD的一个分区的数据依赖于旧RDD【一个分区】的数据  https://raw.githubusercontent.com/kanseavegs/picgo_images/master/data/20211014155819.png\n *\n *                @DeveloperApi\n *                class OneToOneDependency[T](rdd: RDD[T]) extends NarrowDependency[T](rdd) &#123;  //继承自窄依赖\n *                窄依赖表示每一个父(上游)RDD 的 Partition 最多被子（下游）RDD 的一个 Partition 使用，\n *                窄依赖我们形象的比喻为独生子女\n * oneToMany依赖：新的RDD的一个分区的数据依赖于旧RDD【多个分区】的数据  https://raw.githubusercontent.com/kanseavegs/picgo_images/master/data/20211014155913.png\n *                class ShuffleDependency[K: ClassTag, V: ClassTag, C: ClassTag](  extends Dependency[Product2[K, V]] &#123;//由于窄依赖存在 故相对应的称为宽依赖\n *                宽依赖表示同一个父（上游）RDD 的 Partition 被多个子（下游）RDD 的 Partition 依赖，会引起 Shuffle，总结：宽依赖我们形象的比喻为多生。\n *\n *\n */\n\n\n/**\n *【RDD 阶段划分】：DAG（Directed Acyclic Graph）有向无环图是由点和线组成的拓扑图形，该图形具有方向，不会闭环。例如，DAG 记录了 RDD 的转换过程和任务的阶段。\n *                Shuffle过程会产生阶段划分（因为要等待来自不同RDD的数据），oneToOne整个就是一个阶段\n *【RDD任务划分】：待定。。 P96-98\n */\n\n\n\nobject DependencyRDD &#123;\n  def main(args: Array[String]): Unit = &#123;\n\n    val conf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;AdsRDDdemo&quot;)\n    val sc = new SparkContext(conf)\n    println(&quot;******************************textFile************************&quot;)\n    val lines: RDD[String] = sc.textFile(&quot;datas/word.txt&quot;)\n    println(lines.toDebugString)\n    println(&quot;=============================flatmap==========================&quot;)\n    val words: RDD[String] = lines.flatMap(_.split(&quot; &quot;))\n    println(words.toDebugString)\n    println(&quot;******************************map************************&quot;)\n    val wordToOne: RDD[(String, Int)] = words.map(word =&gt; (word, 1))\n    println(wordToOne.toDebugString)\n    println(&quot;==========================reduceByKey==========================&quot;)\n    val wordToSum: RDD[(String, Int)] = wordToOne.reduceByKey(_ + _)\n    println(wordToSum.toDebugString)\n    wordToSum.collect().foreach(println)\n    sc.stop()\n  &#125;\n\n&#125;\n\n</code></pre>\n",
            "tags": [
                "Spark"
            ]
        },
        {
            "id": "http://spark.kanseaveg.xyz/2021/10/22/SparkCore-RDD%E8%A1%8C%E5%8A%A8%E7%AE%97%E5%AD%90/",
            "url": "http://spark.kanseaveg.xyz/2021/10/22/SparkCore-RDD%E8%A1%8C%E5%8A%A8%E7%AE%97%E5%AD%90/",
            "title": "SparkCore-RDD行动算子",
            "date_published": "2021-10-22T06:07:10.000Z",
            "content_html": "<h3 id=\"sparkcore-rdd行动算子\"><a class=\"markdownIt-Anchor\" href=\"#sparkcore-rdd行动算子\">#</a> SparkCore-RDD 行动算子</h3>\n<pre><code>package com.kanseaveg.rdd\n\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.&#123;SparkConf, SparkContext&#125;\n\n/**\n * 行动算子：触发作业执行\n *      底层实际调用的是 sc.runJob(this, (iter: Iterator[T]) =&gt; iter.toArray)\n *       =&gt; dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)\n *       =&gt; val waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties)\n *       =&gt; eventProcessLoop.post(JobSubmitted(jobId, rdd, func2, partitions.toArray, callSite, waiter ....\n *       =&gt; private[scheduler] def handleJobSubmitted(jobId: Int,....\n *       =&gt; val job = new ActiveJob(jobId, finalStage, callSite, listener, properties) 创建activeJob 并提交执行\n *\n */\nobject ActionRDD &#123;\n  def main(args: Array[String]): Unit = &#123;\n\n    val conf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;AdsRDDdemo&quot;)\n    val sc = new SparkContext(conf)\n    val baseRdd: RDD[Int] = sc.makeRDD(List(1, 2, 3, 4),2)\n\n    /**\n     * reduce 聚集 RDD 中的所有元素\n     */\n//    println(baseRdd.reduce(_ + _)) //10\n\n    /**\n     * collect: 将不同分区的数据按照分区顺序采集到Driver端内存中，形成数组\n     */\n//    println(baseRdd.collect().mkString(&quot;,&quot;)) //1,2,3,4\n\n    /**\n     * count: 统计数据源中数据的个数\n     * first： 获取数据源中数据的第一个\n     * take: 获取数据源中N个数据\n     * takeOrdered: 获取数据源中排序后的N个数据\n     */\n//    println(baseRdd.count()) //4\n//    println(baseRdd.first()) //1\n//    println(baseRdd.take(3).mkString(&quot;,&quot;)) //1,2,3\n//    println(baseRdd.takeOrdered(3).mkString(&quot;,&quot;)) //1,2,3\n\n    /**\n     * aggregate: 分区的数据通过初始值和分区内的数据进行聚合，然后再和初始值进行分区间的数据聚合\n     *      aggregateByKey vs aggregate\n     *      aggregateByKey: 初始值只会参与分区内计算  (10+3) + (10+7) = 30\n     *      aggregate： 初始值会参与分区内和分区间的计算 (10+3) + (10+7) + 10 = 40\n     * fold: aggregate简化版：aggregate分区内和分区间规则相同时，进行简化\n     */\n//    println(baseRdd.aggregate(10)(_ + _, _ + _)) //40\n//    println(baseRdd.fold(10)(_ + _))  //40\n\n    /**\n     * countByKey: 统计每种 key 的个数\n     * countByValue: 统计每种 value 的个数\n     */\n//    println(baseRdd.countByValue()) // Map(4 -&gt; 1, 2 -&gt; 1, 1 -&gt; 1, 3 -&gt; 1)\n//    println(sc.makeRDD(List((&quot;a&quot;, 1), (&quot;a&quot;, 20), (&quot;a&quot;, 10), (&quot;b&quot;, 66))).countByKey())  //Map(a -&gt; 3, b -&gt; 1)\n\n    /**\n     * 小小wordcount Sample: 8个实现wordcount方法\n     * (Hello,2)\n     * (Spark,1)\n     * (Scala,1)\n     */\n//    sc.makeRDD(List(&quot;Hello Scala&quot;,&quot;Hello Spark&quot;)).flatMap(_.split(&quot; &quot;)).groupBy(word=&gt;word).mapValues(_.size).collect().foreach(println)\n//    sc.makeRDD(List(&quot;Hello Scala&quot;,&quot;Hello Spark&quot;)).flatMap(_.split(&quot; &quot;)).map((_,1)).groupByKey().mapValues(_.size).collect().foreach(println) //存在shuffle 效率不高\n//    sc.makeRDD(List(&quot;Hello Scala&quot;,&quot;Hello Spark&quot;)).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).collect().foreach(println)\n//    sc.makeRDD(List(&quot;Hello Scala&quot;,&quot;Hello Spark&quot;)).flatMap(_.split(&quot; &quot;)).map((_,1)).aggregateByKey(0)(_+_,_+_).collect().foreach(println)\n//    sc.makeRDD(List(&quot;Hello Scala&quot;,&quot;Hello Spark&quot;)).flatMap(_.split(&quot; &quot;)).map((_,1)).foldByKey(0)(_+_).collect().foreach(println)\n//    sc.makeRDD(List(&quot;Hello Scala&quot;,&quot;Hello Spark&quot;)).flatMap(_.split(&quot; &quot;)).map((_,1)).combineByKey(v=&gt;v,(x:Int,y:Int)=&gt;x+y,(x:Int,y:Int)=&gt;x+y).collect().foreach(println)\n//    sc.makeRDD(List(&quot;Hello Scala&quot;,&quot;Hello Spark&quot;)).flatMap(_.split(&quot; &quot;)).map((_,1)).countByKey().foreach(println)\n//    sc.makeRDD(List(&quot;Hello Scala&quot;,&quot;Hello Spark&quot;)).flatMap(_.split(&quot; &quot;)).countByValue().foreach(println)\n\n\n    /**\n     * save: 保存文件\n     * def saveAsTextFile(path: String): Unit\n     * def saveAsObjectFile(path: String): Unit\n     * def saveAsSequenceFile : //SparkCore-RDD行动算子规定数据的格式必须是键值类型\n     */\n//    baseRdd.saveAsTextFile(&quot;output&quot;)\n//    baseRdd.saveAsObjectFile(&quot;output&quot;)\n//    baseRdd.map((_,1)).saveAsSequenceFile(&quot;output&quot;)\n\n\n    /**\n     * foreach\n     */\n//    println(&quot;在Driver端内存中执行打印操作&quot;) //https://raw.githubusercontent.com/kanseavegs/picgo_images/master/data/20211013163016.png\n//    baseRdd.collect().foreach(println) //collect进行了分区顺序汇总，打印有序\n//    println(&quot;在Executor端执行打印操作&quot;)  //\n//    baseRdd.foreach(println) //分区直接打印，无序\n\n    /**\n     * PS：算子：Operator（操作）\n     *      RDD的方法和Scala集合对象的方法不一样哈\n     *      集合对象的方法都是在同一个节点的内存中完成的\n     *      RDD方法可以将计算逻辑发送到Executor端（分布式节点）执行\n     *      为了区分不同的处理效果，所以才将RDD的方法称为算子\n     *      RDD的方法外部操作都是在Driver端执行的，而方法内部的逻辑代码都是在Executor端执行的。\n     */\n\n\n  &#125;\n&#125;\n\n</code></pre>\n",
            "tags": [
                "Spark"
            ]
        },
        {
            "id": "http://spark.kanseaveg.xyz/2021/10/22/SparkCore-RDD%E9%94%AE%E5%80%BC%E7%B1%BB%E5%9E%8B/",
            "url": "http://spark.kanseaveg.xyz/2021/10/22/SparkCore-RDD%E9%94%AE%E5%80%BC%E7%B1%BB%E5%9E%8B/",
            "title": "SparkCore-RDD转换算子键值类型",
            "date_published": "2021-10-22T06:07:10.000Z",
            "content_html": "<h3 id=\"sparkcore-rdd转换算子键值类型\"><a class=\"markdownIt-Anchor\" href=\"#sparkcore-rdd转换算子键值类型\">#</a> SparkCore-RDD 转换算子键值类型</h3>\n<pre><code>package com.kanseaveg.rdd\n\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.&#123;SparkConf, SparkContext&#125;\n\n/**\n * 键值类型RDD\n */\nobject KeyValueRDD &#123;\n  def main(args: Array[String]): Unit = &#123;\n    val conf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;SingleValueRDD&quot;)\n    val sc = new SparkContext(conf)\n    val baseRdd: RDD[(String, Int)] = sc.makeRDD(List((&quot;a&quot;, 1), (&quot;a&quot;, 2), (&quot;a&quot;, 3), (&quot;b&quot;, 4)))\n    val sameRdd: RDD[(String, Int)] = sc.makeRDD(List((&quot;a&quot;, 1), (&quot;a&quot;, 2), (&quot;b&quot;, 3), (&quot;b&quot;, 4), (&quot;b&quot;, 5), (&quot;a&quot;, 6)), 2)\n\n\n    /**\n     * partitionBy: 根据指定的分区规则对数据进行重新分区\n     *    RDD[(Int, Int)] =&gt; PairRDDFunctions 隐式转换\n     *    RDD有一个伴生对象，伴生对象中存在：\n     *        implicit def rddToPairRDDFunctions[K, V](rdd: RDD[(K, V)])  (implicit kt: ClassTag[K], vt: ClassTag[V], ord: Ordering[K] = null): PairRDDFunctions[K, V] = &#123; new PairRDDFunctions(rdd) &#125;\n     *    new HashPartitioner 实质是 val rawMod = x % mod 取摸运算\n     *    原本分区： 分区0 (1,1) (2,1)  分区1 (3,1) (4,1)\n     *    partitionBy后分区: 分区0 (2,1) (4,1)  分区1 (1,1) (3,1)\n     *\n     */\n//    sc.makeRDD(List(1, 2, 3, 4),2).map((_, 1)).partitionBy(new HashPartitioner(2)).saveAsTextFile(&quot;output&quot;)\n\n\n    /**\n     * reduceByKey: 将数据按照相同的 Key 对 Value 进行两两聚合  若key只有一个就直接返回\n     */\n    val reduceByKeyRdd: RDD[(String, Int)] = baseRdd.reduceByKey(_ + _)\n//    reduceByKeyRdd.collect.foreach(println)  //(a,6) (b,4)\n\n    /**\n     * groupByKey: 将数据源中相同key的数据分在一个组中，形成对偶元组\n     *        存在shuffle操作，会将数据进行打乱重组  [@href=&quot;https://raw.githubusercontent.com/kanseavegs/picgo_images/master/data/20211013092106.png&quot;]\n     *        而spark中shuffle操作必须要落盘操作，不能在内存中数据等待，会导致内存溢出，所以shuffle性能偏低\n     *\n     *        - groupByKey vs groupBy\n     *          val groupByRdd: RDD[(String, Iterable[(String, Int)])] = baseRdd.groupBy(_._1)\n     *          groupByRdd.collect.foreach(println) //(a,CompactBuffer((a,1), (a,2), (a,3)))    (b,CompactBuffer((b,4)))\n     *\n     *        - groupByKey vs reduceByKey\n     *          从 shuffle 的角度：都存在shuffle操作，落盘效率均影响性能，但是reduceByKey提前在分区内做了预聚合(提前对分区内的数据进行预聚合，使得罗盘的数据量小了)  https://raw.githubusercontent.com/kanseavegs/picgo_images/master/data/20211013092557.png\n     *          从功能的角度：reduceByKey 其实包含分组和聚合的功能。GroupByKey 只能分组\n     */\n    val groupByKeyRdd: RDD[(String, Iterable[Int])] = baseRdd.groupByKey()\n//    groupByKeyRdd.collect.foreach(println)  //(a,CompactBuffer(1, 2, 3))  (b,CompactBuffer(4))\n\n    /**\n     * aggregateByKey: 由于reduceByKey分区内combine和分区间shuffle计算【规则相同】，对于某些业务场景可能不适用(比如：分区内求最大值，分区间求和)\n     *                 因此aggregateByKey可以做到将数据根据【不同的规则】进行分区内计算和分区间计算\n     *                 aggregateByKey存在函数柯里化，需要传递两个参数列表\n     *                 - 第一个参数列表需要传递一个初始值，主要用于当碰见第一个key时，和value进行分区内计算\n     *                    一定要设置一个初始值和第一个值进行比较\n     *                    [@href=&quot;https://raw.githubusercontent.com/kanseavegs/picgo_images/master/data/20211013094155.png&quot;]\n     *                    [@href=&quot;https://raw.githubusercontent.com/kanseavegs/picgo_images/master/data/20211013094304.png&quot;]\n     *                 - 第二个参数列表需要传递2个参数\n     *                    - 第1个参数表示分区内的计算规则\n     *                    - 第2个参数表示分区间的计算规则\n     *\n     */\n    val aggregateByKeyRdd: RDD[(String, Int)] = baseRdd.aggregateByKey(0)(\n      (x, y) =&gt; math.max(x, y),\n      (x, y) =&gt; x + y\n    )\n//    baseRdd.aggregateByKey(0)(math.max(_,_),_+_)\n//    aggregateByKeyRdd.collect.foreach(println) //(a,6), (b,4)\n\n\n    /**\n     * foldByKey: 进行聚合计算时，分区内和分区间计算规则相同时，提供的一种简化方法\n     *            val aggregateByKeySameRuleRdd: RDD[(String, Int)] = baseRdd.aggregateByKey(0)(\n     *             (x, y) =&gt; x + y,\n     *             (x, y) =&gt; x + y\n     *             )\n     */\n    val foldByKeyRdd: RDD[(String, Int)] = baseRdd.foldByKey(0)(_ + _)\n//    foldByKeyRdd.collect.foreach(println)\n\n\n    //aggregateByKey求平均值\n//    val avageRdd: RDD[(String, (Int, Int))] = sameRdd.aggregateByKey((0, 0))( // (0,0) 表示使用元组做初始值，第1个0表示数值初始值，第2个0表示数量初始值\n//      //分区内\n//      (tp, v) =&gt; &#123;\n//        (tp._1 + v, tp._2 + 1)\n//      &#125;, // (相同key的value相加，相同key的数量加一)\n//      //分区间\n//      (tp1, tp2) =&gt; &#123;\n//        (tp1._1 + tp2._1, tp1._2 + tp2._2)\n//      &#125;) //（相同key的分区和相加，相同key的数量叠加）\n//      avageRdd.mapValues( tp =&gt; tp._1 /tp._2 ).collect.foreach(println)//只对map中相同key的value进行操作  (a,3), (b,4)\n\n    /**\n     * combineByKey: 类似于aggregateByKey，转换相同key的第一个数据结构，不用设置初始值\n     */\n//    sameRdd.combineByKey(\n//      //将相同key的第一个数据结构进行结构的转换，实现操作\n//      v =&gt; (v,1),\n//      //分区内\n//      (tp:(Int,Int), v) =&gt; &#123;\n//        (tp._1 + v, tp._2 + 1)\n//      &#125;,\n//      //分区间\n//      (tp1:(Int,Int), tp2:(Int,Int)) =&gt; &#123;\n//        (tp1._1 + tp2._1, tp1._2 + tp2._2)\n//      &#125;).mapValues( tp =&gt; tp._1 /tp._2 ).collect.foreach(println)\n\n      //reduceByKey、foldByKey、aggregateByKey、combineByKey【区别】\n      //reduceByKey: 相同 key 的第一个数据不进行任何计算，分区内和分区间计算规则相同\n      //FoldByKey: 相同 key 的第一个数据和初始值进行分区内计算，分区内和分区间计算规则相同\n      //AggregateByKey：相同 key 的第一个数据和初始值进行分区内计算，分区内和分区间计算规则可以不相同\n      //CombineByKey:当计算时，发现数据结构不满足要求时，可以让第一个数据转换结构。分区内和分区间计算规则不相同\n\n\n    /**\n     * join: 相同 key 对应的所有元素连接在一起\n     * 如果两个数据源中key没有匹配上，则不会出现在结果中\n     * 如果两个数据源中具有相同key，则结果中会产生笛卡尔积\n     */\n    val rdd1: RDD[(String, Int)] = sc.makeRDD(List((&quot;a&quot;, 1), (&quot;b&quot;, 2), (&quot;c&quot;, 3)))\n    val rdd2: RDD[(String, Int)] = sc.makeRDD(List((&quot;a&quot;, 4), (&quot;b&quot;, 5), (&quot;c&quot;, 6)))\n    val joinRdd: RDD[(String, (Int, Int))] = rdd1.join(rdd2)\n//    joinRdd.collect.foreach(println) // (a,(1,4))  (b,(2,5))    (c,(3,6))\n//    rdd1.join(sc.makeRDD(List((&quot;d&quot;, 3), (&quot;a&quot;, 1)))).collect.foreach(println) //(a,(1,1))  如果两个数据源中key没有匹配上，则不会出现在结果中\n\n\n    /**\n     * leftOuterJoin: 类似于 SQL 语句的左外连接\n     * (a,(1,Some(4)))\n     * (b,(2,Some(5)))\n     * (c,(3,None))\n     */\n//    sc.makeRDD(List((&quot;a&quot;, 1), (&quot;b&quot;, 2), (&quot;c&quot;, 3))).leftOuterJoin(sc.makeRDD(List((&quot;a&quot;, 4), (&quot;b&quot;, 5)))).collect.foreach(println)\n\n    /**\n     * rightOuterJoin: 类似于 SQL 语句的右外连接\n     * (a,(Some(1),4))\n     * (b,(Some(2),5))\n     * (c,(None,6))\n     */\n//    sc.makeRDD(List((&quot;a&quot;, 1), (&quot;b&quot;, 2))).rightOuterJoin(sc.makeRDD(List((&quot;a&quot;, 4), (&quot;b&quot;, 5),(&quot;c&quot;, 6)))).collect.foreach(println)\n\n    /**\n     * cogroup: connect + group : 先分组，再连接\n     * (a,(CompactBuffer(1),CompactBuffer(4)))\n     * (b,(CompactBuffer(2),CompactBuffer(5)))\n     * (c,(CompactBuffer(),CompactBuffer(6, 7)))\n     */\n//    sc.makeRDD(List((&quot;a&quot;, 1), (&quot;b&quot;, 2))).cogroup(sc.makeRDD(List((&quot;a&quot;, 4), (&quot;b&quot;, 5), (&quot;c&quot;, 6), (&quot;c&quot;, 7)))).collect.foreach(println)\n\n  &#125;\n&#125;\n\n</code></pre>\n",
            "tags": [
                "Spark"
            ]
        }
    ]
}